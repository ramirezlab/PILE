{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to handling advanced Data Structures with Pandas\n",
    "\n",
    "> **Note:** This book is available in two ways:\n",
    "> 1. Downloading the repository and following the instructions in the file [README.md](https://github.com/ramirezlab/CHEMO/blob/main/README.md)\n",
    "> 2. Clicking here on [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ramirezlab/CHEMO/blob/main/2_PART_TWO/2.1_Dataframes.en.ipynb?hl=es)\n",
    "\n",
    "\n",
    "## Numpy Library\n",
    "\n",
    "*Numpy* is a fundamental Python library for scientific computing. Among its features is the creation of multidimensional arrays, which can be treated as vectors, and is very fast when performing mathematical operations on them. Which makes it a necessary tool for projects with high computing and mathematical calculation requirements and that is why it is so popular in the scientific ecosystem.\n",
    "\n",
    "### Math Operations with Numpy Arrays\n",
    "\n",
    "To get started, we'll import the library and create our first Numpy Array, then affect it with some basic math. It is common to rename the `Numpy` library as `np`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The numpy library is imported. The acronym for the numpy library is usually np.\n",
    "import numpy as np\n",
    "\n",
    "# Create an array of numbers\n",
    "nums = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# print the value of the array\n",
    "nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is worth mentioning that a Numpy array is very similar to a native Python list.** The main difference lies in the type of mathematical ordering operations that can be performed on a library array and the native lists of Python. python.\n",
    "\n",
    "Let's take as a starting point the sum of two `Numpy` arrays, compared to a Python sum of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the sum of the Numpy array\n",
    "nums+nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the list was kept, but its elements were added, and this is the potential and simplicity of Numpy arrays, and because of its vast use in the scientific world.\n",
    "\n",
    "Let's look at equivalent code with a native Python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compressed list with addition of its elements\n",
    "list_nums = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "[x + x for x in list_nums]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't think it's complicated to be creating Python *list comprehensions*, but it would clearly reduce the lines of code in our program using **Numpy** arrays. And it would be impossible to achieve it if we need to add two lists.\n",
    "\n",
    "*Numpy arrays* can also perform the other operations following the same pattern:\n",
    "\n",
    "```markdown\n",
    "<numpy_array> <operation (+, -, *, /)> <numpy_array>\n",
    "```\n",
    "*Let's be careful with division if there are zeros in the denominator, remember that it is not possible to divide by zero, in which case an error would appear*\n",
    "\n",
    "Now, to see the potential of Numpy arrays, what would happen if we tried to add two arrays of size $n$, let's talk arrays of $10,000,000 elements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Numpy library is imported\n",
    "import numpy as np\n",
    "\n",
    "# Two arrays with 1 million elements are created, where each element has a random value between 0-10\n",
    "x = np.random.choice(10, 1000000)\n",
    "y = np.random.choice(10, 1000000)\n",
    "\n",
    "print(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you see it? The multiplication took just milliseconds. And that's where it makes using the library worthwhile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Panda Series\n",
    "\n",
    "Before talking about Dataframes it is important that we do a little review about the **Series**, at the same time we will begin to interact with the Pandas library.\n",
    "\n",
    "**`Pandas`** is a Python library (like `Numpy`), which is characterized by providing data structures that are fast processing, that are easy to express to make working with relational data it's easy and intuitive. [More information here](https://pandas.pydata.org/docs/getting_started/overview.html)\n",
    "\n",
    "What is a Serie? How do you create a Serie? Let's see the answers to these questions:\n",
    "\n",
    "Series are a type of one-dimensional structure *(1D)* very similar to an array, with the characteristic that we can label its data to a certain extent through indices.\n",
    "\n",
    "To study the potential of the series we are going to interact with the `iris.data` file (./data/iris.data) that comes from the data repository of the [UCI Machine Learning](https://archive-beta.ics.uci.edu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# URL for the iris data\n",
    "url = \"https://raw.githubusercontent.com/ramirezlab/CHEMO/main/2_PART_TWO/data/iris.data\"\n",
    "\n",
    "# Fetch the data from the URL\n",
    "response = requests.get(url)\n",
    "iris_data = response.text\n",
    "\n",
    "# Create a DataFrame from the fetched data\n",
    "iris_df = pd.read_csv(StringIO(iris_data), header=None, names=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(iris_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*From now on whenever we want to display the content of a series and/or a Dataframe we will use the `head` method, which only displays the first data and not the entire set.*\n",
    "\n",
    "And that's how easy a basic but powerful data structure is built that gives us the *Pandas* library. Let's explore what we have:\n",
    "\n",
    "- Each row is a one-dimensional array with five elements, $150$ rows were printed, each with a list of 5 elements, plus the enumeration column.\n",
    "\n",
    "- The *series* as well as the lists and arrays of *Numpy* have methods and their manipulation is also achieved through indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example 1: The first data in the series is printed using the head method.\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful method is the `index` method, it returns the list of indices and their range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The array indices are printed, in this case it refers to the first column serie_de_iris.index\n",
    "iris_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to access the elements of position $120$ and up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the elements from position 120 onwards\n",
    "iris_df[120:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a *Series* is a one-dimensional *(1D)* array, each row contains all the information returned by the original dataset, represented in a native *Python* array for manipulation. **Technically this makes it difficult** to manipulate the data within each series (for example, finding the mean of the first column, which leads to us wasting the native functionality of a Series, because we would have to create a new one). series for each column of data, then inefficient series.\n",
    "\n",
    "Let's see an example of how to extract the values of the first column in question, for this, we start loading the original dataset again but this time we only take the first column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# URL for the iris data\n",
    "url = \"https://raw.githubusercontent.com/ramirezlab/CHEMO/main/2_PART_TWO/data/iris.data\"\n",
    "\n",
    "# Fetch the data from the URL\n",
    "response = requests.get(url)\n",
    "iris_data = response.text\n",
    "\n",
    "# The dataset is cleaned and the series is built from the array generated by the `split`\n",
    "iris_dataset = iris_data.split('\\n')\n",
    "\n",
    "# The first column is extracted from the dataset and its data is converted to float type\n",
    "iris_dataset = [float(j[0]) if j[0] != '' else 0 for j in [i.split(',') for i in iris_dataset]]\n",
    "\n",
    "# The series is created with the dataset data\n",
    "iris_series_col_1 = pd.Series(iris_dataset)\n",
    "\n",
    "# Print the series with the first column of the dataset\n",
    "print(iris_series_col_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now if we can find some statistics as the mean of all the values, and we'll use *Numpy* for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The mean of the series is calculated from the mean method of numpy\n",
    "print('Average data size:')\n",
    "np.mean(iris_series_col_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limitations of the *Series (as we saw previously)* we will have to give way to a new data structure, and this is where the *Dataframes* come into play. But how about we first describe the columns coming from our dataset?\n",
    "\n",
    "According to the description found on the page we know that it has the following structure:\n",
    "\n",
    "| No. |      Column          |  Data type    | Possible Values                                   |\n",
    "|-----|:--------------------:|--------------:|:------------------------------------------------|\n",
    "| 1   |    sepal length      |   float/cm    | Positives                                       |\n",
    "| 2   |     sepal width      |   float/cm    | Positives                                       |\n",
    "| 3   |    petal length      |   float/cm    | Positives                                       |\n",
    "| 4   |     petal width      |    float/cm   | Positives                                       |\n",
    "| 5   |        class         |  string/text  | Iris Setosa, Iris Versicolour,  Iris Virginica  |\n",
    "\n",
    "It is time for us to modify the *Series*, and convert it into a *Dataframe* and use its full potential to our advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to DataFrames\n",
    "\n",
    "A *DataFrame* is an $n$ dimensional array of structured data that can store data of different types. Yes, it's like a spreadsheet or a database table.\n",
    "\n",
    "The DataFrame are more common than the Series *(They appear everywhere in scientific computing, in the analysis and visualization of data and among others)* and it is for this reason that they will be studied in practice.\n",
    "\n",
    "Usually you can create a `DataFrame` from different data sources, but this time we will continue using the [`iris`](./data/iris.data) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# URL for the iris data\n",
    "url = \"https://raw.githubusercontent.com/ramirezlab/CHEMO/main/2_PART_TWO/data/iris.data\"\n",
    "\n",
    "# Fetch the data from the URL\n",
    "response = requests.get(url)\n",
    "iris_data = response.text\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "column_names = [\"long_sepal\", \"width_sepal\", \"long_petal\", \"width_petal\", \"class_1\"]\n",
    "df_iris = pd.read_csv(StringIO(iris_data), names=column_names)\n",
    "\n",
    "# Print the first 5 elements of the DataFrame\n",
    "print(df_iris.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: from now on whenever we want to import the content of a local dataset we will use the `read_csv` method of the Pandas library.*\n",
    "\n",
    "As we saw it was very easy to build the `DataFrame`, now, how about we manipulate it? We could answer questions like: how many kinds of iris do we have? What is the average size of the petal length? all classes or one in particular? Let's see how it's done:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to work with the first column, we can easily do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris['long_sepal'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column elements are stored as a `Range`, which can be easily operated on, for example finding the average sepal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example 1: Average Sepal Length regardless of class\n",
    "mean_long_sepal = np.mean(df_iris['long_sepal'])\n",
    "print('Average Iris Petal Size:')\n",
    "print(mean_long_sepal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy, find the average of $150$ rows and with it, give a little more meaning to the dataset.\n",
    "\n",
    "The *Dataframe* have many methods that allow to operate, manipulate, group, filter the data, among others. Let's see some of the most important:\n",
    "\n",
    "* **groupby**: used to group the rows around the value of one of its columns\n",
    "* **assign**: used to add new columns from the values of other\n",
    "* **query**: used to filter the dataset based on some condition *(such as the value of a column)*\n",
    "* **filter**: filters a Dataframe by columns or rows according to their labels in the indexes. *(Kindly like the `query` method but the filter is based on the indices)\n",
    "* **value_counts()**: used to count how many values in a column\n",
    "\n",
    "And now how about we now calculate the number of iris classes we have? Let's see how it's done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: the groupby method is used to group the dataframe by the class column\n",
    "print('The number of classes in the dataset is: ') \n",
    "len(df_iris.groupby('class_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much data there is of each class\n",
    "df_iris['class_1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to give way to the answers of the missing questions, let's see what the mean of the petal length for the Iris-setosa class through the query method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Average Petal Length for the class Iris-setosa\n",
    "df_iris_setosa = df_iris.query(\"class_1 == 'Iris-setosa'\")\n",
    "mean_long_petal = np.mean(df_iris_setosa['long_petal'])\n",
    "print('Average petal size of the Iris-setosa class:')\n",
    "print(mean_long_petal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will use the **Method Chain** to calculate a computed value in a new column using the `assign` method, what does that mean about the Method Chain? on the same object in the same line of code, separating each one by a period `.`\n",
    "  An example is better than a thousand words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Computed ratio of petal size for class Iris-virginica\n",
    "# First the Iris-virginica class elements are filtered, then a new column is created where the width and the length of the petal are divided, finally only the first five elements are shown.\n",
    "df_iris.query('class_1 == \"Iris-virginica\"').assign(petal_proportion=lambda i: i['width_petal'] / i['long_petal']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with this we managed to answer the questions previously proposed. So it's easy to manipulate a *DataFrame* while playing with its data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about DataFrames\n",
    "\n",
    "\n",
    "To continue with the practice, it would be good to give a little review on the most basic and standard manipulation ideas of *DataFrames*. We will do this at the same time that we will find the *standard deviation on the length of the petals* of the class *Iris-versicolor*.\n",
    "\n",
    "This demo, basic manipulation process will be step by step as new columns are added, and for convenience we will work with a portion of the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Calculate the standard deviation of the petal length for the Iris Versicolor class\n",
    "df_iris_versicolor = df_iris.query('class_1 == \"Iris-versicolor\"').copy()\n",
    "\n",
    "# Calculate the mean length of the petal\n",
    "media_long_petal = np.mean(df_iris_versicolor['long_petal'])\n",
    "\n",
    "# A column is created with the subtraction of the average of each length of the petal\n",
    "df_iris_versicolor['long_petal_minus_media'] = df_iris_versicolor['long_petal'] - media_long_petal\n",
    "\n",
    "# Create a column with the difference above the square\n",
    "df_iris_versicolor['long_petal_minus_media_square'] = (df_iris_versicolor['long_petal'] - media_long_petal)**2\n",
    "\n",
    "# Calculate the mean squared\n",
    "media_long_petal_error = np.mean(df_iris_versicolor['long_petal_minus_media_square'])\n",
    "\n",
    "# Calculate standard deviation\n",
    "print(f'DevSt = {np.square(media_long_petal_error)}')\n",
    "\n",
    "# Print the dataset\n",
    "df_iris_versicolor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observed, the calculation was quite overwhelming and that is why the previously exposed methods somehow allow a lot to be achieved in so few lines of code.\n",
    "For example, we can do the same thing much more efficiently with the `np.std()` method. Taking the column 'long_petalo' as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devstd_long_petal = np.std(df_iris_versicolor['long_petal'])\n",
    "print(f'DevSt = {devstd_long_petal}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 1: Acquire data from ChEMBL\n",
    "\n",
    "## Concepts to work\n",
    "**[Uniprot](https://www.uniprot.org/):** It is a database that seeks to provide the scientific community with a comprehensive, high-quality and freely accessible resource of protein sequences and functional information<sup> **1** </sup>.\n",
    "\n",
    "**[ChEMBL](https://www.ebi.ac.uk/chembl/):** It is a database that contains bioactive molecules, it gathers chemical, bioactivity and genomic data<sup> **2** </sup>.\n",
    "\n",
    "**Half-maximal inhibitory concentration (IC50):** Expresses the amount of drug necessary to inhibit a biological process by half the uninhibited value, it is the most widely used measure of drug efficacy or potency<sup> **3** </sup>.\n",
    "\n",
    "**pIC50:** Is the negative logarithm to the base ten of the IC50, when the units of are **molars (M)**. It is used to facilitate the comparison between different IC50. Also, it is important to know that the higher the pIC50 the drug has a greater efficacy or greater potential<sup> **3** </sup>.\n",
    "\n",
    "**Half maximal effective concentration (EC50):** It is the effective concentration to produce 50% of the maximum response, it is used to compare the potencies of the drugs. Also, it is important to know that the lower the EC50, the more potent the drug will be. The negative logarithm in base ten **(pEC50)** is also calculated for this request to facilitate its understanding<sup> **4** </sup>.\n",
    "\n",
    "**Inhibitor constant (Ki):** It is the concentration required to produce half of the maximum inhibition, it is useful to describe the binding affinity of a molecule to a receptor.\n",
    "\n",
    "**SMILES (Simplified Molecular-Input Line-Entry System):** This is a line notation for describing chemical structures using short ASCII strings<sup> **5** </sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "For an investigation we want to identify the compounds that act with a specific target, the glycogen synthase kinase-3 beta protein, which acts as a negative regulator in the hormonal control of glucose homeostasis, Wnt signaling and regulation of transcription factors and microtubules .\n",
    "\n",
    "In this practice we are going to explore and learn about the bioactive compounds of the protein, their structures and some physicochemical characteristics.\n",
    "\n",
    "For which, we will use the information provided by the **ChEMBL** database, which allows us to filter and download the known bioactivity data of the compounds that interact with our target of interest. Later, we will work the data in a ` DataFrame `, which will allow us to easily organize, visualize and manipulate it.\n",
    "\n",
    "The first thing to do is connect to **ChEMBL**, using the **webresource client** library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the ChEMBL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chembl_webresource_client\n",
    "!pip install rdkit\n",
    "\n",
    "from chembl_webresource_client.new_client import new_client # The webresource client library is imported that allows connecting to ChEMBL\n",
    "import pandas as pd\n",
    "import math\n",
    "from rdkit.Chem import PandasTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must be created for API access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = new_client.target\n",
    "compounds = new_client.molecule\n",
    "bioactivities = new_client.activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target data\n",
    "Then, we must look for the ID of the target of interest in the Uniprot database, which in this case is Glycogen synthase kinase-3 beta, ID: [P49841](https://www.uniprot.org/uniprot/P49841)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_id = 'P49841'\n",
    "# It takes only some information from ChEMBL that is of interest\n",
    "target_P49841 = targets.get(target_components__accession=uniprot_id) \\\n",
    "                     .only('target_chembl_id', 'organism', 'pref_name', 'target_type')\n",
    "pd.DataFrame.from_records(target_P49841)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the target of interest\n",
    "target = target_P49841[0]\n",
    "print(f'The target of interest is: {str(target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the ChEMBL-ID\n",
    "chembl_id = target['target_chembl_id']\n",
    "print(f'The ChEMBL-ID of interest is: {chembl_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bioactivity data\n",
    "\n",
    "Now we consult the bioactivity data that is of interest. The steps to follow are:\n",
    "1. Download and filter bioactivities for the target\n",
    "     Bioactivity data will be filtered as follows:\n",
    "         * Type of bioactivity: IC50, EC50, Ki\n",
    "         * Relationship: \"=\"\n",
    "2. Convert the downloaded data into a data frame:\n",
    "     The columns of interest are: `molecule_chembl_id`, `type`, `relation`, `pchembl_value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we download the entire database\n",
    "bioact_temp = bioactivities.filter(target_chembl_id = chembl_id)\\\n",
    "                       .filter(relation = '=') \\\n",
    "                       .only('molecule_chembl_id', 'type', 'relation', 'standard_value', 'standard_units', 'pchembl_value', )\n",
    "df_bioact_temp = pd.DataFrame(bioact_temp)\n",
    "# rearrange the columns\n",
    "df_bioact_temp = df_bioact_temp[['molecule_chembl_id', 'type', 'relation', 'value', 'units', 'pchembl_value']]\n",
    "df_bioact_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we filter by the type of activity desired\n",
    "df_bioact = df_bioact_temp[(df_bioact_temp['type'] == 'IC50') |\n",
    "                              (df_bioact_temp['type'] == 'EC50')|\n",
    "                              (df_bioact_temp['type'] == 'Ki')]\n",
    "print(f'Total data loaded: {len(df_bioact)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first composites of the dataframe\n",
    "df_bioact.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the `.head()` method displays the first five elements of the `dataframe`, however, we can quickly see what elements are in the *realtion* and *type* columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bioact['relation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bioact['type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the *realtion* column has only one type (this is due to the initial database filter), we can remove it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bioact.pop('relation')\n",
    "df_bioact.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear data\n",
    "It is possible that some compounds have missing values and also duplicates, since the same compound may have been tested more than once (we will keep only the one that was tested first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we check how many compounds we have in total\n",
    "ori_len = len(df_bioact)\n",
    "print(f'Total original compounds is: {ori_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove compounds that do not have pChEMBL_value\n",
    "df_bioact = df_bioact.dropna(axis=0, how = 'any')\n",
    "new_len = len(df_bioact)\n",
    "print(f'Total compounds after removing those with missing data: {new_len}')\n",
    "# Subtract the total number of compounds from the total number of compounds by removing those without pChEMBL_value\n",
    "print(f'Total compounds removed {ori_len - new_len}')\n",
    "ori_len = new_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate compounds are removed and we are left with the first tested compound\n",
    "df_bioact = df_bioact.drop_duplicates('molecule_chembl_id', keep = 'first')\n",
    "new_len = len(df_bioact)\n",
    "print(f'Total compounds without duplicates : {new_len}')\n",
    "# Subtract from the total number of compounds by removing those without pChEMBL_value the total number of compounds without duplicates\n",
    "print(f'Total compounds removed {ori_len - new_len}')\n",
    "ori_len = new_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bioact.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have removed some rows we will reset the index so that it is continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bioact.reset_index(drop=True, inplace=True)\n",
    "df_bioact.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize the data\n",
    "Let's arrange the DataFrame from largest to smallest pchembl_value. Notice that the column values are not numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_bioact['pchembl_value'][0],type(df_bioact['pchembl_value'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we must first convert them to type `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bioact['pchembl_value'] = df_bioact['pchembl_value'].astype(float)\n",
    "print(df_bioact['pchembl_value'][0],type(df_bioact['pchembl_value'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed to organize the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange from highest to lowest pchembl_value\n",
    "df_bioact.sort_values(by=\"pchembl_value\", ascending=False, inplace=True)\n",
    "# reset the index\n",
    "df_bioact.reset_index(drop=True, inplace=True)\n",
    "# We print the first data\n",
    "df_bioact.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load the data\n",
    "To continue using the Data Frame in practice without always having to connect to ChEMBL, we are going to save the obtained Data Frame as a file separated by commas (data/composites_uniprot_id.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./data\n",
    "df_bioact.to_csv(f\"./data/compounds_{uniprot_id}.csv\", index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, if we want to use the Dataframe, we can load the saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/ramirezlab/CHEMO/main/2_PART_TWO/data/compounds_P49841.csv'\n",
    "df_bioact = pd.read_csv(url,parse_dates=[0])\n",
    "df_bioact.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compound data\n",
    "\n",
    "Next we are going to obtain the data of the molecules that are stored within each molecule_chembl_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librería necesaria para comunicarse con CHEMBL\n",
    "from chembl_webresource_client.new_client import new_client\n",
    "# libreria para trabajar con base de datos (Dataframes)\n",
    "import pandas as pd\n",
    "\n",
    "# Declaración de variables para instanciar el cliente de CHEMBL y acceder a la base de datos de las moléculas\n",
    "compounds = new_client.molecule\n",
    "\n",
    "# Cargamos el archivo antes guardado\n",
    "uniprot_id = 'P49841'\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/ramirezlab/CHEMO/main/2_PART_TWO/data/compounds_P49841.csv'\n",
    "df_bioact = pd.read_csv(url,parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to get the list of compounds that we define as bioactive\n",
    "list_comp_id = list(df_bioact['molecule_chembl_id'])\n",
    "# Get the structure of each compound\n",
    "compounds_list = compounds.filter(molecule_chembl_id__in = list_comp_id) \\\n",
    "                             .only('molecule_chembl_id','molecule_structures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the structure of the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must convert the obtained list into a dataframe. This may take a few minutes\n",
    "df_comp = pd.DataFrame(compounds_list)\n",
    "# remove duplicates\n",
    "df_comp = df_comp.drop_duplicates('molecule_chembl_id', keep = 'first')\n",
    "# print(f'The total number of compounds is: ' + str(len(dataframe_comp)))\n",
    "print(f'Total of compounds is: {str(len(df_comp))}')\n",
    "df_comp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compounds have different types of representations such as the SMILES, the InChI and the InChI Key. We are only interested in staying with the SMILES since it describes the chemical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use a for loop to iterate through each row (df_comp.iterrows())\n",
    "for i, cmpd in df_comp.iterrows():\n",
    "     if df_comp.loc[i]['molecule_structures'] is not None:\n",
    "         df_comp.loc[i]['molecule_structures'] = cmpd['molecule_structures']['canonical_smiles']\n",
    "\n",
    "df_comp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear salts of smiles\n",
    "If we look closely at the *smiles* representation of each molecule, we can see that some have *salts* that need to be cleaned up.\n",
    "First we will filter the molecules that have salts, usually you can see it in the smileies because they have a dot in the text string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp[df_comp.molecule_structures.str.contains(\"\\.\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, of the 2658 initial compounds, 67 have salts that must be cleaned. We can use a `rdkit` module to clean up salts called `rdkit.Chem.SaltRemover`.\n",
    "Suppose we want to remove the salt from the smile \"CN1CCN(CCO/N=C2C(=C3/C(=O)Nc4cc(Br)ccc43)/Nc3ccccc3/2)CC1.Cl\", the following can be done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from rdkit.Chem.SaltRemover import SaltRemover\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles\n",
    "\n",
    "# The module to remove salts is loaded\n",
    "remover = SaltRemover()\n",
    "# convert the smiley to a mol object\n",
    "mol = MolFromSmiles('Br.CCCCCc1n/c(=N\\Cc2cccnc2)sn1-c1ccccc1')\n",
    "# remove salts (res=molecule, deleted=deleted fragment) smile to a mol object\n",
    "res, deleted = remover.StripMolWithDeleted(mol)\n",
    "# convert the mol object to smiles again and remove whitespace\n",
    "MolToSmiles(res).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create a function that does this process to pass it through the `df_comp` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_salts(smile):\n",
    "    remover = SaltRemover()\n",
    "    mol = MolFromSmiles(smile)\n",
    "    res, deleted = remover.StripMolWithDeleted(mol)\n",
    "    return MolToSmiles(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the function to the smiles column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp['molecule_structures'] = df_comp['molecule_structures'].apply(remover_salts)\n",
    "# Remove whitespace around the edges of each string in the \"molecule_structures\" column\n",
    "df_comp[\"molecule_structures\"] = df_comp[\"molecule_structures\"].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look again for smiles that have salts to verify that the cleaning was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp[df_comp.molecule_structures.str.contains(\"\\.\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already cleaned the salts from the molecules, now we must remove the molecules with empty data. For example, cleaning the molecule \"[Cl-].[Li+]\" removes the two salts and leaves the output empty `('')`. To remove these fields we can use `dropna` and filter for those that are not empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp.dropna(subset=[\"molecule_structures\"], inplace=True)\n",
    "df_comp = df_comp[df_comp['molecule_structures'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we already have the set of clean molecules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the Dataframes\n",
    "\n",
    "Now we have two dataframes that we are going to combine to have all the data in a single dataframe and be able to save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Bioactivity Dataframe, only two columns are filtered\n",
    "df_output = pd.merge(df_bioact[['molecule_chembl_id','pchembl_value']], df_comp, on='molecule_chembl_id')\n",
    "print(f'Total of compounds is: {str(len(df_output))}')\n",
    "df_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns can be renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_output.rename(columns= {'molecule_structures':'smiles'})\n",
    "print(f'Total of compounds is: {str(len(df_output))}')\n",
    "df_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the following rdkit function, it is necessary that all the compounds have SMILES, for this reason we eliminate the compounds without SMILES in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_output[~df_output['smiles'].isnull()]\n",
    "print(f'Total of compounds is: {str(len(df_output))}')\n",
    "df_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw the molecule\n",
    "\n",
    "Let's add a new column to the dataframe with the `.AddMoleculeColumnToFrame` function which converts the molecules contained in \"smilesCol\" into RDKit molecules and adds them to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasTools.AddMoleculeColumnToFrame(df_output, smilesCol='smiles')\n",
    "df_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call any compound to see its 2D representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.ROMol.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the obtained dataframe\n",
    "\n",
    "It will save the dataframe as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./data\n",
    "df_output.to_csv(f\"data/compounds_{uniprot_id}_full.csv\", index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "In this practice, the ChEMBL database was used to obtain data on bioactive compounds against our target of interest. These extracted data in the form of dictionaries and lists were converted into a DataFrame which allows easy visualization of the information obtained. In addition, data from the bioactive compounds were obtained, DataFrames were combined, columns were renamed and a panda tool was used to add a new column to the constructed DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Coudert, E., Gehant, S., De Castro, E., Pozzato, M., Baratin, D., Neto, T., Sigrist, C. J. A., Redaschi, N., Bridge, A., The UniProt Consortium, Bridge, A. J., Aimo, L., Argoud-Puy, G., Auchincloss, A. H., Axelsen, K. B., Bansal, P., Baratin, D., Neto, T. M. B., Blatter, M.-C., … Wang, Y. (2023). Annotation of biologically relevant ligands in UniProtKB using ChEBI. Bioinformatics, 39(1), btac793. https://doi.org/10.1093/bioinformatics/btac793\n",
    "\n",
    "2. Mendez, D., Gaulton, A., Bento, A. P., Chambers, J., De Veij, M., Félix, E., Magariños, M. P., Mosquera, J. F., Mutowo, P., Nowotka, M., Gordillo-Marañón, M., Hunter, F., Junco, L., Mugumbate, G., Rodriguez-Lopez, M., Atkinson, F., Bosc, N., Radoux, C. J., Segura-Cabrera, A., … Leach, A. R. (2019). ChEMBL: Towards direct deposition of bioassay data. Nucleic Acids Research, 47(D1), D930-D940. https://doi.org/10.1093/nar/gky1075\n",
    "\n",
    "3. Aykul, S., & Martinez-Hackert, E. (2016). Determination of half-maximal inhibitory concentration using biosensor-based protein interaction analysis. Analytical Biochemistry, 508, 97-103. https://doi.org/10.1016/j.ab.2016.06.025\n",
    "\n",
    "4. Waller, D. G., & Sampson, A. P. (2018). Principles of pharmacology and mechanisms of drug action. En Medical Pharmacology and Therapeutics (pp. 3-31). Elsevier. https://doi.org/10.1016/B978-0-7020-7167-6.00001-4\n",
    "\n",
    "5. Daylight>cheminformatics. (2022). https://www.daylight.com/smiles/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
