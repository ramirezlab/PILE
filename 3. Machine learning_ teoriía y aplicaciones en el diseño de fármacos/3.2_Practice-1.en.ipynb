{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dffa8c7-44a0-460f-bd92-83112e51b4d1",
   "metadata": {},
   "source": [
    "# Practice 5: Ligand classification model\n",
    "\n",
    "> **Note:** This book is available in two ways:\n",
    "> 1. Downloading the repository and following the instructions in the file [README.md](https://github.com/ramirezlab/CHEMO/blob/main/README.md)\n",
    "> 2. Clicking here on [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ramirezlab/CHEMO/blob/main/3_PART_THREE/3.2_Practice-1.en.ipynb?hl=es)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "Machine learning has established itself as an essential component in data science, enabling computers to learn from data and make decisions or predictions without being cleanly programmed to do so. Within this framework, an algorithm of particular importance is the *RandomForest* classification model.\n",
    "\n",
    "<img src=\"./img/random_forest.png\" width=\"600\" align='right'>\n",
    "\n",
    "The RandomForest model is a supervised learning algorithm that is based on the ensemble method. This method combines several weaker algorithms to form a more powerful and robust model. In the case of RandomForest, a \"forest\" of *decision trees* is created, each perturbing on a random subset of the data <sup> **1** </sup>. The end result is the combination of the predictions from all these individual trees.\n",
    "\n",
    "RandomForest is characterized by being versatile and efficient, capable of handling a large number of features and addressing both classification and regression problems. One of the advantages of this algorithm is that it provides a measure of the importance of the variables, offering valuable information about the model and the data.\n",
    "\n",
    "### Validation Strategy: K-fold Cross Validation\n",
    "\n",
    "Model validation is a crucial step in the development of any machine learning algorithm. Its purpose is to evaluate how well the learned model can generalize to unseen data, that is, it was not used during the training phase. In our practice, we will employ the K-Fold cross-validation strategy.\n",
    "\n",
    "<img src=\"./img/K-fold_Cross_Validation.png\" width=\"500\" align='left'>\n",
    "\n",
    "K-Fold cross validation is a powerful and widely used technique that improves model performance estimation. Instead of dividing the data set once into a training set and a test set, K-Fold cross-validation divides the data set into 'K' distinct subsets. The algorithm is then trained 'K' times, each time using a different subset as the test set and the rest of the subsets as the training set. Finally, the performance of the model is averaged over the 'K' iterations to obtain a more robust estimate <sup> **2** </sup>.\n",
    "\n",
    "The goal is to test the model's ability to predict previously unseen data, detect problems such as overfitting, and assess the generalizability of the model.\n",
    "\n",
    "\n",
    "### Performance measures\n",
    "\n",
    "The choice of performance measures depends on the nature of the problem being addressed. However, there are some common measures that are often useful in evaluating the performance of classification models. To understand and calculate these performance measures, it helps to know their formulas. Before providing the formulas, it is important to note that they are based on the concepts of True Positives (**TP**), False Positives (**FP**), True Negatives (**TN**), and False Negatives (**FN**), which are the four possible categories into which the predictions of our model can be classified. The confusion matrix is useful to differentiate each concept <sup> **3** </sup>:\n",
    "\n",
    "<img src=\"./img/confusion_matrix.png\" width=\"400\">\n",
    "\n",
    "* **Accuracy**: It is the proportion of correct predictions among the total number of predictions made. Although it is an intuitive and easy to understand measure, the accuracy can be misleading if the classes are unbalanced. Accuracy is calculated as the sum of the correct predictions (both positive and negative) divided by the total predictions.\n",
    "    $$Accuracy = \\dfrac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "* **Precision (Precision)**: It is the proportion of positive predictions that were correct. It is a useful measure when false positives are of particular concern. Precision is calculated as the number of true positives divided by the sum of true positives and false positives.\n",
    "    $$Precision = \\dfrac{TP}{TP+FP}$$\n",
    "\n",
    "* **Recall (Sensitivity)**: It is the proportion of real positive cases that the model correctly identified. It is important when false negatives are a concern. The recall is calculated as the number of true positives divided by the sum of true positives and false negatives.\n",
    "    $$Remember = \\dfrac{TP}{TP + FN}$$\n",
    "\n",
    "* **F1 Score (F1 Score)**: It is the harmonic mean of precision and recall. This measure seeks a balance between precision and recall. The F1 score is calculated as the harmonic average of precision and recall.\n",
    "    $$F1_{score} = 2 \\times \\dfrac{Precision \\times Recall}{Precision + Recall}$$\n",
    "\n",
    "* **ROC curve (Receiver Operating Characteristic)** <sup> **3** </sup>: This curve is a graphical representation that illustrates the discriminative capacity of a binary classifier as its discrimination threshold varies. It is created by plotting the true positive rate (Recall) against the false positive rate (1-Specificity), at various threshold levels. A model with perfect predictive power would be located in the upper left corner of the graph, while a random model would follow the diagonal line.\n",
    "\n",
    "* **AUC (Area Under the Curve)**: This metric is calculated as the area under the ROC curve. An AUC of 1.0 denotes a perfect pattern, while an AUC of 0.5 denotes a pattern that has no discriminatory ability, equivalent to a random selection. The higher the AUC, the better the model will be at distinguishing between the positive and negative classes.\n",
    "\n",
    "In our analysis of the implementation of the RandomForest model, we will use these measures to assess its performance and generalizability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfad86e-60f6-41e7-b94e-a741297c3e53",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "We start by importing the data from the previous lab, as these are stored in the folder of the second part, we can create a `root directory` (`ROOT_DIR`) to navigate to the file and load it into a dataframe\n",
    "\n",
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2a431-ae81-4533-8dc5-f5d7c4451012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Libraries needed to import the saved database\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the UniProt ID and the URL for the CSV file\n",
    "uniprot_id = 'P49841'\n",
    "csv_url = 'https://raw.githubusercontent.com/ramirezlab/CHEMO/main/2_PART_TWO/data/compounds_P49841_lipinski.csv'\n",
    "\n",
    "# Read the CSV file from the URL into a DataFrame\n",
    "df_output = pd.read_csv(csv_url)\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "df_output.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7234bb3-0df9-40b1-bcd9-ac609209eeab",
   "metadata": {},
   "source": [
    "In this exercise we only need the ligands that comply with the *rule of five*, therefore, we must filter by the column: `rule_of_five_conform:yes`. Also, we only need the first three columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae3efd-77c5-4bba-a7ff-c06872625fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'# total ligands: {len(df_output)}')\n",
    "df_output = df_output[df_output['rule_of_five_conform']=='yes']\n",
    "df_output = df_output[['molecule_chembl_id', 'pchembl_value', 'smiles']]\n",
    "print(f'# filtered ligands (rule_of_five_conform:yes): {len(df_output)}')\n",
    "\n",
    "df_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacaf7cf-88b5-4d86-bd6d-756ab0964752",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "### Molecular Fingerprints\n",
    "\n",
    "To train our algorithm, it is necessary to convert the ligands into a list of features. Currently, we have the molecular structure (SMILES) of each ligand, and with this information we can generate an alternative representation known as *fingerprint*. This representation will be used later to train the model.\n",
    "\n",
    "To identify and generate the fingerprints of each ligand, we will use the `rdkit` library. This operation will result in the creation of a new column in our data set that will contain the fingerprint of each ligand. There are several types of fingerprints, but this time we will work with the [Extended Connectivity Fingerprint ECFP](https://docs.chemaxon.com/display/docs/extended-connectivity-fingerprint-ecfp.md) also known as morgan2_c/ecfp4 <sup> **4** </sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ba875-9c3c-461a-92d0-97073978d4de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "df_fp = df_output.copy()\n",
    "df_fp['morgan2_c'] = df_output.smiles.map(lambda smile: rdMolDescriptors.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smile), 2).ToList())\n",
    "df_fp = df_fp[['molecule_chembl_id', 'morgan2_c', 'pchembl_value']]\n",
    "df_fp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057c5a31-e420-4a97-97f5-4345dc65442e",
   "metadata": {},
   "source": [
    "Let's explore the first fingerprint: a binary list (ones and zeros) with a length of 2048 elements. These fingerprint elements will be the features that will be used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a7499-ac1a-4117-9e2d-fa42bb0dcb86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df_fp.morgan2_c[0])\n",
    "print(len(df_fp.morgan2_c[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b3924c-fb92-4fd2-ace1-136b4ce91ae3",
   "metadata": {},
   "source": [
    "### Classification of ligands\n",
    "\n",
    "Each ligand must be classified as **active** or **inactive**, for this we will use the `pchembl_value` column defining activity thresholds\n",
    "The protein *Glycogen synthase kinase-3 beta* is classified in the group of *Kinases*, therefore, we will use the following thresholds:\n",
    "\n",
    "**Inactive**: *pchembl_value* < 6.52 uM\n",
    "\n",
    "**Active**: *pchembl_value* >= 7.52 uM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d792e7-2fa7-403c-9f01-e4d26ff9fb78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "# Add column for activity, default = internet\n",
    "df_fp['activity_type'] = 'Intermediate'\n",
    "# Mark each molecule as active with a pIC50 >= 7.52\n",
    "df_fp.loc[df_fp[df_fp.pchembl_value >= 7.5].index, 'activity_type'] = 'Active'\n",
    "# Mark each molecule as inactive with a pIC50 of < 6.52\n",
    "df_fp.loc[df_fp[df_fp.pchembl_value < 6.52].index, 'activity_type'] = 'Inactive'\n",
    "df_fp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd7f59-14f3-4b65-8e7f-13b74eccc429",
   "metadata": {},
   "source": [
    "Let's see graphically how the classification was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d59a8d-702c-4d4d-bdc7-4776aca7c08e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df_fp.activity_type.value_counts())\n",
    "df_fp.activity_type.value_counts().plot.bar(x='activity_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b3c76-4f25-4af1-9427-6e5e19ca87db",
   "metadata": {},
   "source": [
    "Now we filter the data removing those that are classified as *Intermediate*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee1efa-0b00-431f-b8c4-abce67a9fbc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bd = df_fp[df_fp['activity_type'] != 'Intermediate'].copy()\n",
    "bd.activity_type.value_counts().plot.bar(x='activity_type')\n",
    "print(f'# ligandos (active/inactive): {len(bd)}')\n",
    "print(bd.activity_type.value_counts())\n",
    "bd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b78eeb-c55b-4a89-8c50-9ae660f1f4c7",
   "metadata": {},
   "source": [
    "Since it is a binary classification, we must assign a label: (Inactive:0 / Active:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e9d67-5c92-4ba2-b01d-13485661049b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bd['activity'] = 0\n",
    "bd.loc[bd[bd.activity_type == 'Active'].index, 'activity'] = 1.0\n",
    "bd.drop(['activity_type', 'pchembl_value'], axis=1, inplace=True)\n",
    "bd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3383fed-253c-4fc7-aef3-0991cd8cedb3",
   "metadata": {},
   "source": [
    "We already have the features (morgan2_c fingerprint) and tags (activity) to be able to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e538aa1-3ec2-4628-87b6-2a26ec8c4bc3",
   "metadata": {},
   "source": [
    "# Train the model with the *Random Forest* algorithm\n",
    "\n",
    "We are going to train a Random Forest model that classifies ligands knowing the fingerprint. The goal is to test the model's ability to predict data that has never been seen before, to detect problems known as overfitting, and to assess the generalizability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672be5c2-8bf2-4476-bc4d-47977e37f297",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Forest\n",
    "Usually, the first step is to **split** the data set, one part for training (70%) and the other part for testing (30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdcc2c0-d04c-43ce-a67e-bf8d691bfe06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "fp_df_train, fp_df_test = train_test_split(bd, test_size=0.3, random_state=142857,\n",
    "                                             shuffle=True, stratify=bd['activity'])\n",
    "fp_df_train.reset_index(drop=True, inplace=True)\n",
    "fp_df_test.reset_index(drop=True, inplace=True)\n",
    "print(f'# training data: {len(fp_df_train)},'\n",
    "       f'\\n# test data: {len(fp_df_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c533ee9-2a7c-49ad-9e70-5def2ec79c9d",
   "metadata": {},
   "source": [
    "Now, for each set we are going to separate the characteristics (the fingerprint) and the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab551f43-54e1-49c5-8ee3-3f5a085ebc0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_train = fp_df_train.morgan2_c, fp_df_train.activity\n",
    "X_test, y_test = fp_df_test.morgan2_c, fp_df_test.activity\n",
    "# The array of features should be converted to a list of items\n",
    "X_train, X_test = X_train.tolist(), X_test.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d008968-cbb2-40ea-a591-72d252861bb6",
   "metadata": {},
   "source": [
    "We choose the estimator of [Random Fores classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to train the model, the model must be instantiated and built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bee019-1b60-4253-bcba-dd255e36e384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a350863-69de-4afc-b00f-742eb2643fe8",
   "metadata": {},
   "source": [
    "## Validation\n",
    "### Accuracy score\n",
    "\n",
    "There are several metrics to measure the ability of the model to make predictions, let's see an example using the metric [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html?highlight=accuracy_score#sklearn.metrics.accuracy_score)\n",
    "\n",
    "The first thing is to classify (*predict*) the data from the set and then compare it with the true labels, we will do this with both the **training set** and the **test set**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b4f66-987a-4401-8c2a-af2a32f4a81b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Prediction training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "# prediction set validation\n",
    "y_test_pre = model.predict(X_test)\n",
    "\n",
    "# Punctuation\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "acc_test = accuracy_score(y_test, y_test_pre)\n",
    "print(f'Accuracy training set: {acc_train:.4f} ({acc_train:.2%})\\n'\n",
    "       f'Accuracy test set: {acc_test:.4f} ({acc_test:.2%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45113ee6-eb9f-4437-9af1-953524f88ab8",
   "metadata": {},
   "source": [
    "The *accuracy* of the training set is 100%, which indicates a case of *Overfitting*, it may be necessary to adjust the parameters of the classification model or even use another model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8d7f9d-8faa-4e40-a980-353806eaa75a",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "With this matrix you can compare the true labels versus the model predictions, [here](https://en.wikipedia.org/wiki/Confusion_matrix) you can see more information about the confusion matrix. In this case we are going to compare the data from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507cc185-3f43-49d7-bb1a-4bb9b815b453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pre, colorbar=False,  cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00db0f-c1fb-4831-a681-9145834b7452",
   "metadata": {},
   "source": [
    "You can work with the normalized data to see it as a percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ccdd4e-71fc-48a7-bbd8-1bad409578f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pre, colorbar=False,\n",
    "                                        cmap=plt.cm.Blues, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67159fc6-6bab-4db8-9895-96e3a5d4a242",
   "metadata": {},
   "source": [
    "### ROC curve\n",
    "The ROC curve (ROC curve, Receiver Operating Characteristic) is a graphical representation of the sensitivity versus the specificity for a binary classifier system as the discrimination threshold is varied, it is usually used to represent how good the model is, let's see how you can build one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8406dfb3-c307-4092-84de-39d973f42a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Probability of predictions\n",
    "pred_prob_train = model.predict_proba(X_train)[:, 1]\n",
    "pred_prob_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# false positive rates / true positive rates - training\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, pred_prob_train)\n",
    "roc_auc_train = auc(fpr_train, tpr_train)\n",
    "# false positive rates / true positive rates - test\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, pred_prob_test)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(fpr_train, tpr_train, label=f'AUC train = {roc_auc_train:.2f}', lw=2)\n",
    "plt.plot(fpr_test, tpr_test, label=f'AUC test = {roc_auc_test:.2f}', lw=2)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Random', lw=2, color=\"black\") # Random curve\n",
    "\n",
    "plt.xlabel('False positive rate', size=24)\n",
    "plt.ylabel('True positive rate', size=24)\n",
    "plt.title('Random forest ROC curves', size=24)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48899a94-a9eb-4b41-880a-4e10e315777a",
   "metadata": {},
   "source": [
    "### K-fold (cross validation)\n",
    "\n",
    "We are going to divide the data into 5 sets, each one of them will train the algorithm and measure its predictive capacity, then the data from the five models will be contrasted to validate if the trained model works or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93890f4-d263-4ec3-bf0b-26088a273f03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "n_folds = 5\n",
    "# empty result vector\n",
    "results = []\n",
    "# Shuffle the indices for k-fold cross-validation\n",
    "kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "# Labels initialized with -1 for each data point\n",
    "labels = -1 * np.ones(len(bd))\n",
    "# instance model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "for train_index, test_index in kf.split(bd):\n",
    "    # Training\n",
    "    # Converts the bit vector and the label to a list\n",
    "    train_x = bd.iloc[train_index].morgan2_c.tolist()\n",
    "    train_y = bd.iloc[train_index].activity.tolist()\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    # Evidence\n",
    "    # Converts the bit vector and the label to a list\n",
    "    test_x = bd.iloc[test_index].morgan2_c.tolist()\n",
    "    test_y = bd.iloc[test_index].activity.tolist()\n",
    "    # Predict on test set\n",
    "    prediction_prob = model.predict_proba(test_x)[:, 1]\n",
    "    # Save the predicted label of each fold\n",
    "    labels[test_index] = model.predict(test_x)\n",
    "\n",
    "    # performance\n",
    "    # Get fpr, tpr and roc_auc for each fold\n",
    "    fpr_l, tpr_l, _ = roc_curve(test_y, prediction_prob)\n",
    "    roc_auc_l = auc(fpr_l, tpr_l)\n",
    "    # Add to results\n",
    "    results.append((fpr_l, tpr_l, roc_auc_l))\n",
    "# Get overall accuracy, sensitivity, specificity\n",
    "y = bd.activity.tolist()\n",
    "acc = accuracy_score(y, labels)\n",
    "sens = recall_score(y, labels)\n",
    "spec = (acc * len(y) - sens * sum(y)) / (len(y) - sum(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad711c-f24b-486d-a41b-783be09fe273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "cmap = mpl.colormaps['Blues']\n",
    "colors = [cmap(i) for i in np.linspace(0.1, 1.0, n_folds)]\n",
    "\n",
    "for i, (fpr_l, tpr_l, roc_auc_l) in enumerate(results):\n",
    "    plt.plot(fpr_l, tpr_l, label='AUC CV$_{0}$ = {1:0.2f}'.format(str(i),roc_auc_l), lw=2, color=colors[i])\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Random', lw=2, color=\"black\")  # Random curve\n",
    "plt.xlabel('False positive rate', size=24)\n",
    "plt.ylabel('True positive rate', size=24)\n",
    "plt.title(f'Random forest ROC curves', size=24)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d9fb42-a504-411a-95db-2f55f19f87aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the mean AUC and print\n",
    "m_auc = np.mean([elem[2] for elem in results])\n",
    "print(f'Mean AUC: {m_auc:.3f}')\n",
    "\n",
    "# show overall precision, sensitivity, specificity\n",
    "print(f'Sensitivity: {sens:.3f}\\nAccuracy: {acc:.3f}\\nSpecificity: {spec:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b76ccc3-11cc-40b4-8832-4425abf44549",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "The Random Forest classification algorithm is exceptionally powerful for performing binary classifications. In the case of our study, this involved classifying molecules as active or inactive. However, our initial implementation of the model revealed a significant overfitting of the data. This phenomenon suggests that the algorithm tries to capture all the characteristics of the molecules instead of achieving an effective generalization. Overfitting can lead to low predictability for molecules that are not part of the training set, a scenario we would prefer to avoid.\n",
    "\n",
    "## Practice Activity\n",
    "To decrease the overfitting problem, you can change the [model parameters](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier), which regulate how the training is carried out. As a practical activity, you are invited to experiment with the modification of these parameters and to compare the results obtained. Can you find a set of parameters that will reduce overfitting and improve the overall performance of the model? How do these changes affect different performance metrics? Explore and share your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185a51c-f29a-4f4c-8725-1c14b230199c",
   "metadata": {},
   "source": [
    "# Machine Learning XGBoost\n",
    "\n",
    "An alternative to the overfitting problem is to explore the use of a different classification model to train our algorithm. This time, we decided to implement XGBoost, an efficient and sophisticated machine learning software library based on the decision tree boosting algorithm <sup> **5** </sup>.\n",
    "\n",
    "XGBoost, which stands for eXtreme Gradient Boosting, is known for its speed and performance. It is an algorithm that has proven valuable in a variety of data science competitions and has been widely adopted in the industry. As a boosting algorithm, XGBoost is based on the idea of creating a strong predictive model by combining a series of weaker models, iteratively improving the predictions of the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9116fb8d-ec69-423e-8a34-24bd45bb3e4e",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "In this practice we are going to train a model with some parameters established in advance, which improve the default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1360dfef-6cf0-4be7-b3b0-53a01dd65201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgbc_model = xgb.XGBClassifier()\n",
    "# training parameters\n",
    "params = {'objective': 'binary:logistic', 'early_stopping_rounds': 20,\n",
    "           'enable_categorical': False, 'eval_metric': ['error', 'auc'],\n",
    "           'gamma': 0.2, 'grow_policy': 'depthwise',\n",
    "           'learning_rate': 0.32, 'max_depth': 7,\n",
    "           'min_child_weight': 1, 'n_estimators': 100,\n",
    "           'scale_pos_weight': 3.271, 'subsample': 0.8,\n",
    "           'alpha': 0.2, 'lambda': 1.4}\n",
    "xgbc_model.set_params(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decc7d8e-5da6-4681-95d1-7904cfdccdbb",
   "metadata": {},
   "source": [
    "To train the XGBoost model it is necessary to generate a validation set to use the \"early stopping\" functionality of XGBoost. \"Early stopping\" is a way to prevent model overfitting by stopping training when the validation error stops improving.\n",
    "\n",
    "Thus, our initial set `X_train, y_train` is divided into two: `X_temp, y_temp` and `X_valid, y_valid` with a ratio of 80:20.\n",
    "\n",
    "Then we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743f709e-237b-4ed8-bbef-b49958b7266f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_temp, X_valid, y_temp, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "xgbc_model.fit(X_temp, y_temp, eval_set=[(X_valid, y_valid)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bdf822-0cbe-4bcf-b8b7-55eeb23a1221",
   "metadata": {},
   "source": [
    "## Validation of the model\n",
    "With the model already trained as `xgbc_model`, we can now evaluate the performance metrics, for example the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf3fd70-e3e2-45bd-97ce-bc6c859844f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction training set\n",
    "y_train_pred = xgbc_model.predict(X_train)\n",
    "# test set prediction\n",
    "y_test_pre = xgbc_model.predict(X_test)\n",
    "\n",
    "# Punctuation\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "acc_test = accuracy_score(y_test, y_test_pre)\n",
    "print(f'Accuracy training set: {acc_train:.4f} ({acc_train:.2%})\\n'\n",
    "       f'Accuracy test set: {acc_test:.4f} ({acc_test:.2%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f201baf-1183-439b-bbbc-e7e05b477ba3",
   "metadata": {},
   "source": [
    "This result indicates that the overfitting has been decreased.\n",
    "\n",
    "Let's look at the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf7083-775a-48c3-88e6-63dd656edf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pre, colorbar=False,  cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1941488-610d-4568-aa59-945b57fcfd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pre, colorbar=False,\n",
    "                                        cmap=plt.cm.Blues, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2467af71-e8ec-4f9b-9190-54befe06b1a5",
   "metadata": {},
   "source": [
    "The process can also be repeated to plot the ROC curve along with the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11573dda-f69c-4048-ae69-f14c765d9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of predictions\n",
    "pred_prob_train = xgbc_model.predict_proba(X_train)[:, 1]\n",
    "pred_prob_test = xgbc_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# false positive rates / true positive rates - training\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, pred_prob_train)\n",
    "roc_auc_train = auc(fpr_train, tpr_train)\n",
    "# false positive rates / true positive rates - test\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, pred_prob_test)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(fpr_train, tpr_train, label=f'AUC train = {roc_auc_train:.2f}', lw=2)\n",
    "plt.plot(fpr_test, tpr_test, label=f'AUC test = {roc_auc_test:.2f}', lw=2)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Random', lw=2, color=\"black\") # Random curve\n",
    "\n",
    "plt.xlabel('False positive rate', size=24)\n",
    "plt.ylabel('True positive rate', size=24)\n",
    "plt.title('XGBoost ROC curves', size=24)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3777af4-4e48-4a59-ba53-04bebff535f9",
   "metadata": {},
   "source": [
    "# Practical Activity\n",
    "\n",
    "Taking into account what was reviewed in this first part, make a code in python with which you can:\n",
    "\n",
    "1. Randomly search for 10 compounds from the ChemBL database. You will need to calculate the smiles and determine if the compound is going to be active or not.\n",
    "\n",
    "At the end, you must prepare a document in PDF format in which you attach the proposed code and the output of the execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7dceb4-1c56-48c9-b7dd-3cbaa3fff1a7",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "Throughout this practice, we work with two widely used classification algorithms: Random Forest and XGBoost. Each of these algorithms has its own advantages and limitations, and their performance can vary greatly depending on the type of data and parameter settings.\n",
    "\n",
    "Our first approach was to employ the Random Forest algorithm using the default parameters. Although Random Forest is known for its ability to handle a wide range of classification problems, we found that in our case the resulting model suffered from overfitting. Overfitting is a common phenomenon in machine learning, where a model memorizes the features of the training set instead of learning to generalize from the underlying features. This limits the model's ability to make accurate predictions on unseen data.\n",
    "\n",
    "To address this issue, we experimented with a second algorithm: XGBoost. XGBoost is a powerful and flexible algorithm that can be especially effective in addressing overfitting issues if configured correctly. For our XGBoost model, we defined an initial set of parameters and observed that after training and validating the model, the overfitting had decreased.\n",
    "\n",
    "It's crucial to remember that there is no overall \"best\" or \"worst\" ranking algorithm. The effectiveness of an algorithm depends to a large extent on the data with which it works and how its parameters are configured. Therefore, the process of finding the most suitable classification algorithm for a given problem usually involves experimenting with different models and adjusting their parameters. Ultimately, the choice of algorithm and its configuration is a trade-off between model performance, interpretability, and computational efficiency.\n",
    "\n",
    "As a continuation of this practice, it would be interesting to explore other classification models, as well as experiment with different parameter tuning techniques, such as grid search or Bayesian optimization, to further improve the performance of our classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023bd36a-3fcb-4082-b542-4e32a33a5919",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Sarica, A., Cerasa, A., & Quattrone, A. (2017). Random forest algorithm for the classification of neuroimaging data in alzheimer’s disease: A systematic review. Frontiers in Aging Neuroscience, 9. https://www.frontiersin.org/articles/10.3389/fnagi.2017.00329\n",
    "2. Refaeilzadeh, P., Tang, L., & Liu, H. (2009). Cross-validation. En L. LIU & M. T. ÖZSU (Eds.), Encyclopedia of Database Systems (pp. 532-538). Springer US. https://doi.org/10.1007/978-0-387-39940-9_565\n",
    "3. Larrañaga, P., Calvo, B., Santana, R., Bielza, C., Galdiano, J., Inza, I., Lozano, J. A., Armañanzas, R., Santafé, G., Pérez, A., & Robles, V. (2006). Machine learning in bioinformatics. Briefings in Bioinformatics, 7(1), 86-112. https://doi.org/10.1093/bib/bbk007\n",
    "4. Extended connectivity fingerprint ecfp | chemaxon docs. (s. f.). https://docs.chemaxon.com/display/docs/extended-connectivity-fingerprint-ecfp.md\n",
    "5. Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 785–794). New York, NY, USA: ACM. https://doi.org/10.1145/2939672.2939785"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
