{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dffa8c7-44a0-460f-bd92-83112e51b4d1",
   "metadata": {},
   "source": [
    "# Practice 5: Ligand classification model\n",
    "\n",
    "> **Note:** This book is available in two ways:\n",
    "> 1. Downloading the repository and following the instructions in the file [README.md](https://github.com/ramirezlab/CHEMO/blob/main/README.md)\n",
    "> 2. Clicking here on [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ramirezlab/PILE/blob/main/3.%20Machine%20learning%3A%20teor%C3%ADa%20y%20aplicaciones%20en%20el%20dise%C3%B1o%20de%20f%C3%A1rmacos/3.2_Practice-1.en.ipynb?hl=es)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "Machine learning has established itself as an essential component in data science, enabling computers to learn from data and make decisions or predictions without being cleanly programmed to do so. Within this framework, an algorithm of particular importance is the *RandomForest* classification model.\n",
    "\n",
    "<img src=\"./img/random_forest.png\" width=\"600\" align='right'>\n",
    "\n",
    "The RandomForest model is a supervised learning algorithm that is based on the ensemble method. This method combines several weaker algorithms to form a more powerful and robust model. In the case of RandomForest, a \"forest\" of *decision trees* is created, each perturbing on a random subset of the data <sup> **1** </sup>. The end result is the combination of the predictions from all these individual trees.\n",
    "\n",
    "RandomForest is characterized by being versatile and efficient, capable of handling a large number of features and addressing both classification and regression problems. One of the advantages of this algorithm is that it provides a measure of the importance of the variables, offering valuable information about the model and the data.\n",
    "\n",
    "### Validation Strategy: K-fold Cross Validation\n",
    "\n",
    "Model validation is a crucial step in the development of any machine learning algorithm. Its purpose is to evaluate how well the learned model can generalize to unseen data, that is, it was not used during the training phase. In our practice, we will employ the K-Fold cross-validation strategy.\n",
    "\n",
    "<img src=\"./img/K-fold_Cross_Validation.png\" width=\"500\" align='left'>\n",
    "\n",
    "K-Fold cross validation is a powerful and widely used technique that improves model performance estimation. Instead of dividing the data set once into a training set and a test set, K-Fold cross-validation divides the data set into 'K' distinct subsets. The algorithm is then trained 'K' times, each time using a different subset as the test set and the rest of the subsets as the training set. Finally, the performance of the model is averaged over the 'K' iterations to obtain a more robust estimate <sup> **2** </sup>.\n",
    "\n",
    "The goal is to test the model's ability to predict previously unseen data, detect problems such as overfitting, and assess the generalizability of the model.\n",
    "\n",
    "\n",
    "### Performance measures\n",
    "\n",
    "The choice of performance measures depends on the nature of the problem being addressed. However, there are some common measures that are often useful in evaluating the performance of classification models. To understand and calculate these performance measures, it helps to know their formulas. Before providing the formulas, it is important to note that they are based on the concepts of True Positives (**TP**), False Positives (**FP**), True Negatives (**TN**), and False Negatives (**FN**), which are the four possible categories into which the predictions of our model can be classified. The confusion matrix is useful to differentiate each concept <sup> **3** </sup>:\n",
    "\n",
    "<img src=\"./img/confusion_matrix.png\" width=\"400\">\n",
    "\n",
    "* **Accuracy**: It is the proportion of correct predictions among the total number of predictions made. Although it is an intuitive and easy to understand measure, the accuracy can be misleading if the classes are unbalanced. Accuracy is calculated as the sum of the correct predictions (both positive and negative) divided by the total predictions.\n",
    "    $$Accuracy = \\dfrac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "* **Precision (Precision)**: It is the proportion of positive predictions that were correct. It is a useful measure when false positives are of particular concern. Precision is calculated as the number of true positives divided by the sum of true positives and false positives.\n",
    "    $$Precision = \\dfrac{TP}{TP+FP}$$\n",
    "\n",
    "* **Recall (Sensitivity)**: It is the proportion of real positive cases that the model correctly identified. It is important when false negatives are a concern. The recall is calculated as the number of true positives divided by the sum of true positives and false negatives.\n",
    "    $$Remember = \\dfrac{TP}{TP + FN}$$\n",
    "\n",
    "* **F1 Score (F1 Score)**: It is the harmonic mean of precision and recall. This measure seeks a balance between precision and recall. The F1 score is calculated as the harmonic average of precision and recall.\n",
    "    $$F1_{score} = 2 \\times \\dfrac{Precision \\times Recall}{Precision + Recall}$$\n",
    "\n",
    "* **ROC curve (Receiver Operating Characteristic)** <sup> **3** </sup>: This curve is a graphical representation that illustrates the discriminative capacity of a binary classifier as its discrimination threshold varies. It is created by plotting the true positive rate (Recall) against the false positive rate (1-Specificity), at various threshold levels. A model with perfect predictive power would be located in the upper left corner of the graph, while a random model would follow the diagonal line.\n",
    "\n",
    "* **AUC (Area Under the Curve)**: This metric is calculated as the area under the ROC curve. An AUC of 1.0 denotes a perfect pattern, while an AUC of 0.5 denotes a pattern that has no discriminatory ability, equivalent to a random selection. The higher the AUC, the better the model will be at distinguishing between the positive and negative classes.\n",
    "\n",
    "In our analysis of the implementation of the RandomForest model, we will use these measures to assess its performance and generalizability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfad86e-60f6-41e7-b94e-a741297c3e53",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "We start by importing the data from the previous lab, as these are stored in the folder of the second part, we can create a `root directory` (`ROOT_DIR`) to navigate to the file and load it into a dataframe\n",
    "\n",
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2a431-ae81-4533-8dc5-f5d7c4451012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The Pandas library is imported with the alias 'pd'\n",
    "import pandas as pd\n",
    "\n",
    "# The 'os' module is imported for file system operations\n",
    "import os\n",
    "\n",
    "# 'Path' is imported from the 'pathlib' library to work with file paths more flexibly\n",
    "from pathlib import Path\n",
    "\n",
    "# The 'requests' library is imported to perform HTTP requests\n",
    "import requests\n",
    "\n",
    "# Define the UniProt ID and the URL of the CSV file\n",
    "uniprot_id = 'P49841'\n",
    "csv_url = 'https://raw.githubusercontent.com/ramirezlab/PILE/refs/heads/main/2.%20De%20datos%20a%20gr%C3%A1ficas%3A%20Propiedades%20drug-likeness%20y%20similitud%20qu%C3%ADmica%20con%20python/data/compounds_P49841_lipinski.csv'\n",
    "\n",
    "# Read the CSV file from the URL and load it into a DataFrame\n",
    "df_output = pd.read_csv(csv_url)\n",
    "\n",
    "# Display the first rows of the DataFrame\n",
    "df_output.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7234bb3-0df9-40b1-bcd9-ac609209eeab",
   "metadata": {},
   "source": [
    "In this exercise we only need the ligands that comply with the *rule of five*, therefore, we must filter by the column: `rule_of_five_conform:yes`. Also, we only need the first three columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae3efd-77c5-4bba-a7ff-c06872625fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the total number of ligands in the original DataFrame\n",
    "print(f'# lignados totales: {len(df_output)}')\n",
    "\n",
    "# Filter the DataFrame to keep only ligands that comply with Lipinski's rule (rule_of_five_conform == 'yes')\n",
    "df_output = df_output[df_output['rule_of_five_conform']=='yes']\n",
    "\n",
    "# Select only the relevant columns: ligand ID, pChEMBL value, and SMILES string\n",
    "df_output = df_output[['molecule_chembl_id', 'pchembl_value', 'smiles']]\n",
    "\n",
    "# Print the number of ligands that comply with Lipinski's rule\n",
    "print(f'# ligandos filtrados (rule_of_five_conform:yes): {len(df_output)}')\n",
    "\n",
    "# Display the first rows of the filtered DataFrame\n",
    "df_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacaf7cf-88b5-4d86-bd6d-756ab0964752",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "### Molecular Fingerprints\n",
    "\n",
    "To train our algorithm, it is necessary to convert the ligands into a list of features. Currently, we have the molecular structure (SMILES) of each ligand, and with this information we can generate an alternative representation known as *fingerprint*. This representation will be used later to train the model.\n",
    "\n",
    "To identify and generate the fingerprints of each ligand, we will use the `rdkit` library. This operation will result in the creation of a new column in our data set that will contain the fingerprint of each ligand. There are several types of fingerprints, but this time we will work with the [Extended Connectivity Fingerprint ECFP](https://docs.chemaxon.com/display/docs/extended-connectivity-fingerprint-ecfp.md) also known as morgan2_c/ecfp4 <sup> **4** </sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ba875-9c3c-461a-92d0-97073978d4de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the RDKit library (if not already installed)\n",
    "!pip install rdkit\n",
    "\n",
    "# Import the 'Chem' module from RDKit to manipulate molecules\n",
    "from rdkit import Chem\n",
    "\n",
    "# Import the 'rdMolDescriptors' module from RDKit to generate molecular descriptors\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "\n",
    "# Create a copy of the original DataFrame to work with fingerprints\n",
    "df_fp = df_output.copy()\n",
    "\n",
    "# Compute the Morgan fingerprint (radius 2) for each molecule from its SMILES string\n",
    "# 'Chem.MolFromSmiles(smile)' converts the SMILES string into a Mol object\n",
    "# 'GetMorganFingerprintAsBitVect(..., 2)' generates the fingerprint as a binary vector\n",
    "# 'ToList()' converts the fingerprint to a list of integers (0s and 1s)\n",
    "df_fp['morgan2_c'] = df_output.smiles.map(lambda smile: rdMolDescriptors.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smile), 2).ToList())\n",
    "\n",
    "# Select only the relevant columns: ligand ID, Morgan fingerprint, and pChEMBL value\n",
    "df_fp = df_fp[['molecule_chembl_id', 'morgan2_c', 'pchembl_value']]\n",
    "\n",
    "# Display the first rows of the DataFrame with fingerprints\n",
    "df_fp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057c5a31-e420-4a97-97f5-4345dc65442e",
   "metadata": {},
   "source": [
    "Let's explore the first fingerprint: a binary list (ones and zeros) with a length of 2048 elements. These fingerprint elements will be the features that will be used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a7499-ac1a-4117-9e2d-fa42bb0dcb86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the Morgan fingerprint (as a list of 0s and 1s) corresponding to the first ligand in the DataFrame\n",
    "print(df_fp.morgan2_c[0])\n",
    "\n",
    "# Print the length of the first ligand's fingerprint (total number of bits in the vector)\n",
    "print(len(df_fp.morgan2_c[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b3924c-fb92-4fd2-ace1-136b4ce91ae3",
   "metadata": {},
   "source": [
    "### Classification of ligands\n",
    "\n",
    "Each ligand must be classified as **active** or **inactive**, for this we will use the `pchembl_value` column defining activity thresholds\n",
    "The protein *Glycogen synthase kinase-3 beta* is classified in the group of *Kinases*, therefore, we will use the following thresholds:\n",
    "\n",
    "**Inactive**: *pchembl_value* < 6.52 uM\n",
    "\n",
    "**Active**: *pchembl_value* >= 7.52 uM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d792e7-2fa7-403c-9f01-e4d26ff9fb78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add a new column called 'activity_type' with default value 'Intermediate'\n",
    "df_fp['activity_type'] = 'Intermediate'\n",
    "\n",
    "# Mark as 'Active' those molecules with a pChEMBL value greater than or equal to 7.5\n",
    "df_fp.loc[df_fp[df_fp.pchembl_value >= 7.5].index, 'activity_type'] = 'Active'\n",
    "\n",
    "# Mark as 'Inactive' those molecules with a pChEMBL value less than 6.52\n",
    "df_fp.loc[df_fp[df_fp.pchembl_value < 6.52].index, 'activity_type'] = 'Inactive'\n",
    "\n",
    "# Display the first rows of the updated DataFrame\n",
    "df_fp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd7f59-14f3-4b65-8e7f-13b74eccc429",
   "metadata": {},
   "source": [
    "Let's see graphically how the classification was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d59a8d-702c-4d4d-bdc7-4776aca7c08e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the number of molecules classified in each activity category (Active, Intermediate, Inactive)\n",
    "print(df_fp.activity_type.value_counts())\n",
    "\n",
    "# Generate a bar plot showing the distribution of molecules by activity type\n",
    "df_fp.activity_type.value_counts().plot.bar(x='activity_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b3c76-4f25-4af1-9427-6e5e19ca87db",
   "metadata": {},
   "source": [
    "Now we filter the data removing those that are classified as *Intermediate*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee1efa-0b00-431f-b8c4-abce67a9fbc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A new DataFrame 'bd' is created containing only the molecules classified as 'Active' or 'Inactive'\n",
    "# Molecules labeled as 'Intermediate' are excluded, and a copy of the subset is made\n",
    "bd = df_fp[df_fp['activity_type'] != 'Intermediate'].copy()\n",
    "\n",
    "# A bar plot is generated showing the number of active and inactive ligands\n",
    "bd.activity_type.value_counts().plot.bar(x='activity_type')\n",
    "\n",
    "# Print the total number of ligands in the new DataFrame (only active and inactive)\n",
    "print(f'# ligandos (active/inactive): {len(bd)}')\n",
    "\n",
    "# Print the count of ligands by activity category (Active, Inactive)\n",
    "print(bd.activity_type.value_counts())\n",
    "\n",
    "# Display the first rows of the 'bd' DataFrame\n",
    "bd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b78eeb-c55b-4a89-8c50-9ae660f1f4c7",
   "metadata": {},
   "source": [
    "Since it is a binary classification, we must assign a label: (Inactive:0 / Active:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e9d67-5c92-4ba2-b01d-13485661049b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A new column called 'activity' is added and initialized with value 0 (by default all molecules are inactive)\n",
    "bd['activity'] = 0\n",
    "\n",
    "# The value 1.0 is assigned in the 'activity' column to molecules classified as 'Active'\n",
    "bd.loc[bd[bd.activity_type == 'Active'].index, 'activity'] = 1.0\n",
    "\n",
    "# The columns 'activity_type' and 'pchembl_value' are removed from the DataFrame, as they will not be used in the model\n",
    "bd.drop(['activity_type', 'pchembl_value'], axis=1, inplace=True)\n",
    "\n",
    "# Display the first rows of the updated DataFrame\n",
    "bd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3383fed-253c-4fc7-aef3-0991cd8cedb3",
   "metadata": {},
   "source": [
    "We already have the features (morgan2_c fingerprint) and tags (activity) to be able to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e538aa1-3ec2-4628-87b6-2a26ec8c4bc3",
   "metadata": {},
   "source": [
    "# Train the model with the *Random Forest* algorithm\n",
    "\n",
    "We are going to train a Random Forest model that classifies ligands knowing the fingerprint. The goal is to test the model's ability to predict data that has never been seen before, to detect problems known as overfitting, and to assess the generalizability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672be5c2-8bf2-4476-bc4d-47977e37f297",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Forest\n",
    "Usually, the first step is to **split** the data set, one part for training (70%) and the other part for testing (30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdcc2c0-d04c-43ce-a67e-bf8d691bfe06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The 'train_test_split' function is imported from the 'model_selection' module of scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The 'bd' DataFrame is split into training and test sets\n",
    "# 'test_size=0.3' indicates that 30% of the data will be used for testing\n",
    "# 'random_state=142857' ensures the reproducibility of the split\n",
    "# 'shuffle=True' shuffles the data randomly before splitting\n",
    "# 'stratify=bd['activity']' ensures that the class proportions (0 and 1) are preserved in both sets\n",
    "fp_df_train, fp_df_test = train_test_split(bd, test_size=0.3, random_state=142857,\n",
    "                                            shuffle=True, stratify=bd['activity'])\n",
    "\n",
    "# The indices of the training and test DataFrames are reset to avoid duplicates or disordered indices\n",
    "fp_df_train.reset_index(drop=True, inplace=True)\n",
    "fp_df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# The number of samples in the training and test sets is printed\n",
    "print(f'# datos entrenamiento: {len(fp_df_train)},'\n",
    "      f'\\n# datos prueba: {len(fp_df_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c533ee9-2a7c-49ad-9e70-5def2ec79c9d",
   "metadata": {},
   "source": [
    "Now, for each set we are going to separate the characteristics (the fingerprint) and the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab551f43-54e1-49c5-8ee3-3f5a085ebc0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The input variables (X) and the target variable (y) are separated for the training set\n",
    "X_train, y_train = fp_df_train.morgan2_c, fp_df_train.activity\n",
    "\n",
    "# The input variables (X) and the target variable (y) are separated for the test set\n",
    "X_test, y_test = fp_df_test.morgan2_c, fp_df_test.activity\n",
    "\n",
    "# The feature vectors are converted to lists of elements for use with scikit-learn models\n",
    "X_train, X_test = X_train.tolist(), X_test.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d008968-cbb2-40ea-a591-72d252861bb6",
   "metadata": {},
   "source": [
    "We choose the estimator of [Random Fores classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to train the model, the model must be instantiated and built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bee019-1b60-4253-bcba-dd255e36e384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The 'RandomForestClassifier' class is imported from the 'ensemble' module of scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# An instance of the Random Forest classification model is created\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# The model is trained using the training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a350863-69de-4afc-b00f-742eb2643fe8",
   "metadata": {},
   "source": [
    "## Validation\n",
    "### Accuracy score\n",
    "\n",
    "There are several metrics to measure the ability of the model to make predictions, let's see an example using the metric [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html?highlight=accuracy_score#sklearn.metrics.accuracy_score)\n",
    "\n",
    "The first thing is to classify (*predict*) the data from the set and then compare it with the true labels, we will do this with both the **training set** and the **test set**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b4f66-987a-4401-8c2a-af2a32f4a81b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The 'accuracy_score' function is imported from the 'metrics' module of scikit-learn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# The model's prediction is performed on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# The model's prediction is performed on the test (validation) set\n",
    "y_test_pre = model.predict(X_test)\n",
    "\n",
    "# Accuracy is calculated for the training set\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Accuracy is calculated for the test set\n",
    "acc_test = accuracy_score(y_test, y_test_pre)\n",
    "\n",
    "# Accuracy scores are printed with 4 decimal places and as percentages\n",
    "print(f'Accuracy conjunto de entrenamiento: {acc_train:.4f} ({acc_train:.2%})\\n'\n",
    "      f'Accuracy conjunto de prueba: {acc_test:.4f} ({acc_test:.2%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45113ee6-eb9f-4437-9af1-953524f88ab8",
   "metadata": {},
   "source": [
    "The *accuracy* of the training set is 100%, which indicates a case of *Overfitting*, it may be necessary to adjust the parameters of the classification model or even use another model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8d7f9d-8faa-4e40-a980-353806eaa75a",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "With this matrix you can compare the true labels versus the model predictions, [here](https://en.wikipedia.org/wiki/Confusion_matrix) you can see more information about the confusion matrix. In this case we are going to compare the data from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507cc185-3f43-49d7-bb1a-4bb9b815b453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The 'ConfusionMatrixDisplay' class is imported from scikit-learn's 'metrics' module to visualize confusion matrices\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# The Matplotlib library is imported for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The confusion matrix is generated and displayed based on the test set predictions\n",
    "# 'colorbar=False' disables the color bar\n",
    "# 'cmap=plt.cm.Blues' sets a blue color palette for the visualization\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pre, colorbar=False,  cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00db0f-c1fb-4831-a681-9145834b7452",
   "metadata": {},
   "source": [
    "You can work with the normalized data to see it as a percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ccdd4e-71fc-48a7-bbd8-1bad409578f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The normalized confusion matrix is generated and displayed based on the test set predictions\n",
    "# 'colorbar=False' disables the color bar\n",
    "# 'cmap=plt.cm.Blues' sets a blue-toned color palette\n",
    "# 'normalize=\"true\"' normalizes the matrix by row, showing proportions instead of absolute counts\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pre, colorbar=False,\n",
    "                                        cmap=plt.cm.Blues, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67159fc6-6bab-4db8-9895-96e3a5d4a242",
   "metadata": {},
   "source": [
    "### ROC curve\n",
    "The ROC curve (ROC curve, Receiver Operating Characteristic) is a graphical representation of the sensitivity versus the specificity for a binary classifier system as the discrimination threshold is varied, it is usually used to represent how good the model is, let's see how you can build one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8406dfb3-c307-4092-84de-39d973f42a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The 'roc_curve' and 'auc' functions are imported from scikit-learn's 'metrics' module to evaluate model performance\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# Matplotlib libraries are imported for visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The predicted probabilities for the positive class (1) are obtained from the training set\n",
    "pred_prob_train = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# The predicted probabilities for the positive class (1) are obtained from the test set\n",
    "pred_prob_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# False positive rates (FPR) and true positive rates (TPR) are calculated for the training set\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, pred_prob_train)\n",
    "# The area under the curve (AUC) is calculated for the training set\n",
    "roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "# False positive rates (FPR) and true positive rates (TPR) are calculated for the test set\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, pred_prob_test)\n",
    "# The area under the curve (AUC) is calculated for the test set\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# A figure is created to plot the ROC curves\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "# The ROC curve for the training set is plotted\n",
    "plt.plot(fpr_train, tpr_train, label=f'AUC train = {roc_auc_train:.2f}', lw=2)\n",
    "\n",
    "# The ROC curve for the test set is plotted\n",
    "plt.plot(fpr_test, tpr_test, label=f'AUC test = {roc_auc_test:.2f}', lw=2)\n",
    "\n",
    "# A diagonal line is plotted as a reference for a random classifier\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Random', lw=2, color=\"black\")  # Random curve\n",
    "\n",
    "# Axis labels and title are configured\n",
    "plt.xlabel('False positive rate', size=24)\n",
    "plt.ylabel('True positive rate', size=24)\n",
    "plt.title('Random forest ROC curves', size=24)\n",
    "\n",
    "# Axis tick label size is adjusted\n",
    "plt.tick_params(labelsize=16)\n",
    "\n",
    "# The plot legend is displayed\n",
    "plt.legend(fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48899a94-a9eb-4b41-880a-4e10e315777a",
   "metadata": {},
   "source": [
    "### K-fold (cross validation)\n",
    "\n",
    "We are going to divide the data into 5 sets, each one of them will train the algorithm and measure its predictive capacity, then the data from the five models will be contrasted to validate if the trained model works or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93890f4-d263-4ec3-bf0b-26088a273f03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "n_folds = 5\n",
    "# Empty list to store results\n",
    "results = []\n",
    "\n",
    "# Shuffle indices for k-fold cross-validation\n",
    "kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "\n",
    "# Labels initialized to -1 for each data point\n",
    "labels = -1 * np.ones(len(bd))\n",
    "\n",
    "# Model instance\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "for train_index, test_index in kf.split(bd):\n",
    "    # Training\n",
    "    # Convert binary vectors and labels to lists\n",
    "    train_x = bd.iloc[train_index].morgan2_c.tolist()\n",
    "    train_y = bd.iloc[train_index].activity.tolist()\n",
    "\n",
    "    # Fit the model with the training data\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    # Testing\n",
    "    # Convert binary vectors and labels to lists\n",
    "    test_x = bd.iloc[test_index].morgan2_c.tolist()\n",
    "    test_y = bd.iloc[test_index].activity.tolist()\n",
    "\n",
    "    # Predict probabilities on the test set\n",
    "    prediction_prob = model.predict_proba(test_x)[:, 1]\n",
    "\n",
    "    # Store the predicted label for each fold\n",
    "    labels[test_index] = model.predict(test_x)\n",
    "\n",
    "    # Evaluation\n",
    "    # Get FPR, TPR, and AUC for each fold\n",
    "    fpr_l, tpr_l, _ = roc_curve(test_y, prediction_prob)\n",
    "    roc_auc_l = auc(fpr_l, tpr_l)\n",
    "\n",
    "    # Add fold results to the list\n",
    "    results.append((fpr_l, tpr_l, roc_auc_l))\n",
    "\n",
    "# Compute overall metrics: accuracy, sensitivity, and specificity\n",
    "y = bd.activity.tolist()\n",
    "acc = accuracy_score(y, labels)\n",
    "sens = recall_score(y, labels)\n",
    "spec = (acc * len(y) - sens * sum(y)) / (len(y) - sum(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad711c-f24b-486d-a41b-783be09fe273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A figure is created for plotting with size 7x7 inches\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "# A blue color map is retrieved from Matplotlib\n",
    "cmap = mpl.colormaps['Blues']\n",
    "\n",
    "# A list of colors is generated from the color map, spaced evenly according to the number of folds\n",
    "colors = [cmap(i) for i in np.linspace(0.1, 1.0, n_folds)]\n",
    "\n",
    "# The list of results from cross-validation is iterated over\n",
    "for i, (fpr_l, tpr_l, roc_auc_l) in enumerate(results):\n",
    "    # The ROC curve for each fold is plotted with its respective AUC\n",
    "    plt.plot(fpr_l, tpr_l, label='AUC CV$_{0}$ = {1:0.2f}'.format(str(i), roc_auc_l), lw=2, color=colors[i])\n",
    "    # The x-axis limits are defined\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    # The y-axis limits are defined\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "\n",
    "# A diagonal line is plotted to represent a random classifier\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Random', lw=2, color=\"black\")  # Random curve\n",
    "\n",
    "# The x-axis label is set\n",
    "plt.xlabel('False positive rate', size=24)\n",
    "\n",
    "# The y-axis label is set\n",
    "plt.ylabel('True positive rate', size=24)\n",
    "\n",
    "# The title of the plot is set\n",
    "plt.title(f'Random forest ROC curves', size=24)\n",
    "\n",
    "# The size of axis tick labels is adjusted\n",
    "plt.tick_params(labelsize=16)\n",
    "\n",
    "# The plot legend is displayed\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "# The plot is displayed on screen\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d9fb42-a504-411a-95db-2f55f19f87aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the mean AUC from the fold results and print it\n",
    "m_auc = np.mean([elem[2] for elem in results])\n",
    "print(f'Mean AUC: {m_auc:.3f}')\n",
    "\n",
    "# Display overall metrics: sensitivity, accuracy, and specificity\n",
    "print(f'Sensitivity: {sens:.3f}\\nAccuracy: {acc:.3f}\\nSpecificity: {spec:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b76ccc3-11cc-40b4-8832-4425abf44549",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "The Random Forest classification algorithm is exceptionally powerful for performing binary classifications. In the case of our study, this involved classifying molecules as active or inactive. However, our initial implementation of the model revealed a significant overfitting of the data. This phenomenon suggests that the algorithm tries to capture all the characteristics of the molecules instead of achieving an effective generalization. Overfitting can lead to low predictability for molecules that are not part of the training set, a scenario we would prefer to avoid.\n",
    "\n",
    "## Practice Activity\n",
    "To decrease the overfitting problem, you can change the [model parameters](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier), which regulate how the training is carried out. As a practical activity, you are invited to experiment with the modification of these parameters and to compare the results obtained. Can you find a set of parameters that will reduce overfitting and improve the overall performance of the model? How do these changes affect different performance metrics? Explore and share your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7dceb4-1c56-48c9-b7dd-3cbaa3fff1a7",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "Throughout this practice, we work with two widely used classification algorithms: Random Forest and XGBoost. Each of these algorithms has its own advantages and limitations, and their performance can vary greatly depending on the type of data and parameter settings.\n",
    "\n",
    "Our first approach was to employ the Random Forest algorithm using the default parameters. Although Random Forest is known for its ability to handle a wide range of classification problems, we found that in our case the resulting model suffered from overfitting. Overfitting is a common phenomenon in machine learning, where a model memorizes the features of the training set instead of learning to generalize from the underlying features. This limits the model's ability to make accurate predictions on unseen data.\n",
    "\n",
    "To address this issue, we experimented with a second algorithm: XGBoost. XGBoost is a powerful and flexible algorithm that can be especially effective in addressing overfitting issues if configured correctly. For our XGBoost model, we defined an initial set of parameters and observed that after training and validating the model, the overfitting had decreased.\n",
    "\n",
    "It's crucial to remember that there is no overall \"best\" or \"worst\" ranking algorithm. The effectiveness of an algorithm depends to a large extent on the data with which it works and how its parameters are configured. Therefore, the process of finding the most suitable classification algorithm for a given problem usually involves experimenting with different models and adjusting their parameters. Ultimately, the choice of algorithm and its configuration is a trade-off between model performance, interpretability, and computational efficiency.\n",
    "\n",
    "As a continuation of this practice, it would be interesting to explore other classification models, as well as experiment with different parameter tuning techniques, such as grid search or Bayesian optimization, to further improve the performance of our classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023bd36a-3fcb-4082-b542-4e32a33a5919",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Sarica, A., Cerasa, A., & Quattrone, A. (2017). Random forest algorithm for the classification of neuroimaging data in alzheimer’s disease: A systematic review. Frontiers in Aging Neuroscience, 9. https://www.frontiersin.org/articles/10.3389/fnagi.2017.00329\n",
    "2. Refaeilzadeh, P., Tang, L., & Liu, H. (2009). Cross-validation. En L. LIU & M. T. ÖZSU (Eds.), Encyclopedia of Database Systems (pp. 532-538). Springer US. https://doi.org/10.1007/978-0-387-39940-9_565\n",
    "3. Larrañaga, P., Calvo, B., Santana, R., Bielza, C., Galdiano, J., Inza, I., Lozano, J. A., Armañanzas, R., Santafé, G., Pérez, A., & Robles, V. (2006). Machine learning in bioinformatics. Briefings in Bioinformatics, 7(1), 86-112. https://doi.org/10.1093/bib/bbk007\n",
    "4. Extended connectivity fingerprint ecfp | chemaxon docs. (s. f.). https://docs.chemaxon.com/display/docs/extended-connectivity-fingerprint-ecfp.md\n",
    "5. Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 785–794). New York, NY, USA: ACM. https://doi.org/10.1145/2939672.2939785"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
