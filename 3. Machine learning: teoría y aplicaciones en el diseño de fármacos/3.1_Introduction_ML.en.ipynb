{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "> **Note:** This book is available in two ways:\n",
    "> 1. Downloading the repository and following the instructions in the file [README.md](https://github.com/ramirezlab/CHEMO/blob/main/README.md)\n",
    "> 2. Clicking here on [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ramirezlab/CHEMO/blob/main/3_PART_THREE/3.1_Introduction_ML.en.ipynb?hl=es)\n",
    "\n",
    "\n",
    "Machine Learning is a field of computing that uses algorithms to allow machines to learn from data, without being explicitly programmed to do so <sup> **1** </sup>. It is one of the foundations of modern technologies, powering everything from search engines to voice and object recognition systems.\n",
    "\n",
    "In this notebook, learning algorithms will be used to recognize patterns in a given data set. After being trained, these algorithms will be able to make accurate predictions for new data, thanks to the adjustment of their parameters during the training process.\n",
    "\n",
    "There is a wide variety of ML algorithms, each suitable for different types of problems. In general, these algorithms are grouped into three main categories: Supervised Learning, Unsupervised Learning, and Reinforcement Learning. Each of them has its own characteristics and particular applications.\n",
    "\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "Supervised Learning is the process by which labeled data is used to train an ML model. This labeled data consists of a set of observations and their corresponding results, and is used to teach the model to accurately predict future results <sup> **1, 2** </sup>. This is a highly efficient process, and is commonly used in classification and regression tasks.\n",
    "\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "Unsupervised Learning is an ML technique used when no labeled data is available. Instead of using labeled observations, the ML model explores the underlying patterns in the data and uses them to make predictions <sup> **1, 2** </sup>. This technique is commonly used in customer segmentation and anomaly detection.\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning involves teaching the ML model how to make decisions in a dynamic environment, through trial and error. The ML model uses the feedback given by your environment to adjust your behavior and improve your future decisions <sup> **1, 2** </sup>. This technique is commonly used in robotics and computer games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression Models\n",
    "\n",
    "Classification and regression models are two Supervised Learning techniques widely used in the field of Machine Learning. These models allow predicting an outcome from a set of input variables, using previously provided training data.\n",
    "\n",
    "## Regression Models\n",
    "Regression is a type of Supervised Learning that is used to predict continuous values <sup> **1** </sup>. In other words, it is used to predict a value based on one or more input variables.\n",
    "\n",
    "For example, consider the house price dataset, where the goal is to predict the price of a house based on characteristics such as area, number of rooms, etc. In this case, Regression is used to predict a continuous value (the house price) based on the input variables.\n",
    "\n",
    "There are many different types of Regression, including Linear Regression, Polynomial Regression, and Logistic Regression. Each type of Regression has its own assumptions and limitations, and the choice of a Regression technique depends on the specific problem being solved.\n",
    "\n",
    "In the field of bioinformatics, these models can be used to predict the relationship between the amount of certain chemicals in a cell and its response to different situations. Furthermore, these models can be used in physics and chemistry to predict how a chemical reaction may change as certain external factors change. Some common regression algorithms include Linear Regression, Polynomial Regression, and Robust Regression.\n",
    "\n",
    "## Classification Models\n",
    "Classification is another type of Supervised Learning that is used to predict discrete values (labels) <sup> **1** </sup>. In other words, it is used to predict a label based on one or more input variables.\n",
    "\n",
    "For example, consider the data set containing information about cars, where the goal is to predict whether a car is \"safe\" or \"not safe\" based on characteristics such as size, power, etc. In this case, Classification is used to predict a discrete label (secure or not secure) based on the input variables.\n",
    "\n",
    "There are many different types of Classification, including Decision Trees, Support Vector Machines (SVMs), and Neural Networks. Each type of Classification has its own assumptions and limitations, and also the choice of a Classification technique depends on the specific problem being solved.\n",
    "\n",
    "Classification models are commonly used in bioinformatics to classify different types of DNA or protein sequences. In addition, they are vital tools in the field of cell biology to help identify different types of cells in a tissue. In environmental sciences, classification models are used to classify different types of animals or plants. Some common classification algorithms include Decision Trees, Logistic Regression, and Support Vector Machines.\n",
    "\n",
    "## Classification Models vs Regression Models\n",
    "While classification models focus on predicting categorical values, regression models focus on predicting numerical values. This means that classification models are used to solve classification problems, while regression models are used for numerical value prediction problems.\n",
    "\n",
    "In conclusion, classification and regression models are fundamental techniques in the field of bioinformatics and science in general. These models allow you to predict accurate results from pre-provided input data sets. For students of these disciplines, it is important to have a good understanding of these techniques and how to apply them to different problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first machine learning model\n",
    "\n",
    "The idea now is to go step by step developing a very simple machine learning model. During this process we will make use of the `Pandas` and `Numpy` libraries and we will introduce a new one called `sklearn`.\n",
    "\n",
    "Let's start by downloading the data we are going to work with. For the development of our model we are going to use the classic [iris dataset](https://archive-beta.ics.uci.edu/dataset/53/iris):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas and give it the alias pd\n",
    "import pandas as pd\n",
    "\n",
    "# Import the data from the dataset and convert it to a DataFrame.\n",
    "df_iris = pd.read_csv(\"https://raw.githubusercontent.com/ramirezlab/CHEMO/main/3_PART_THREE/data/iris.csv\", names=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"])\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what we have in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 150 observations in total (`count`), the set has four features (columns: `sepal_length`, `sepal_width`, `petal_length` and `petal width`) and an iris class classification (column `class`), Furthermore, since the characteristics of the set are numerical, we can see some relevant statistics such as the mean, standard deviation and some percentile data. Beyond that we see that our set is very well organized and at first glance does not seem to have any value that is going to be a problem. Can we know how many kinds of iris were observed? Of course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `value_counts()` method tells us that there are 3 different classes of iris, each with respectively 50 observations (this is known as a *balanced* data set). Let's see it graphically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Seaborn library and assign it the alias sns\n",
    "import seaborn as sns\n",
    "\n",
    "# The class column of the DataFrame is displayed\n",
    "sns.countplot(x=df_iris['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're in luck, our data is already balanced, and to a large extent this is important in a *classification* problem, but in real life it's more realistic to deal with unbalanced data sets.\n",
    "\n",
    "> If the data is not balanced it will lead us to unbalanced models that would have very low performance or predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations and more\n",
    "\n",
    "Next, we'll use the power of visualization to find out if we have any patterns, or correlations, in some of the iris features over the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn is imported, and the data is visualized in a pairplot.\n",
    "import seaborn as sns\n",
    "sns.pairplot(df_iris, hue=\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the graphs a bit, we can see that most of the characteristics *(Petal width and length, sepal length and width) * follow a **normal distribution**, which means that most of the observations are located at the center of the data, through this reading we could think about what would be the best algorithm for this problem.\n",
    "\n",
    "We can also see that some Iris classes, and in certain features the data does not overlap, which tells us that it is okay for us to use as many columns as possible to try to avoid some phony data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the next step is about the correlations that may exist between the characteristics and for this we will use the **heat map**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figure, axis = plt.subplots()\n",
    "\n",
    "sns.heatmap(df_iris.corr(numeric_only=True), annot=True, fmt=\".2f\", ax=axis)\n",
    "\n",
    "plt.title('Iris Correlation Map')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analyzing the data set, it has been identified that the independent characteristics, such as the length and width of the petal or the length and width of the sepal, present a high degree of correlation. This phenomenon is known as multiple correlation and can lead to unreliable results when performing inferential statistical analyses.\n",
    "\n",
    "In this particular case, the `petal_length` feature has been identified as having a fairly high correlation with other features. For this reason, to avoid possible problems due to multiple correlation, it is recommended to remove `petal_length` from the data set.\n",
    "\n",
    "It is important to perform a detailed analysis of the independent features to identify any possible correlations and determine the appropriate solutions to reduce the complexity of the data set. In some cases, removing highly correlated features can result in a significant decrease in complexity, which can improve the quality of the results when performing data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.drop('petal_length', axis=1, inplace=True)\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Algorithm: Random Forest Classifier\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Random Forest algorithm is used in supervised learning for classification, regression, and other data mining tasks. This assembly technique combines several independent decision trees to create an enhanced singular model.\n",
    "\n",
    "\n",
    "## How does the Random Forest algorithm work?\n",
    "To understand the way the Random Forest algorithm works, one must first consider a simpler model, the decision tree. The latter uses “if/then” logic to make decisions, dividing the data set into smaller subsets using a series of simple questions, known as “nodes”, to arrive at a final prediction. Thus, from a training data set, logical rules are used to predict the target value of new sets of features. The Random Forest algorithm implements this model, but with multiple decision trees that randomly take a part of the training data set and then vote on the ranking <sup> **1** </sup>. To illustrate the steps of the algorithm, they are described in detail in the following section.\n",
    "\n",
    "1. *Selection of training data*: A random sample of size N is taken from the original training data, which is called Bootstrap random sampling. From that sample a decision tree is created.\n",
    "2. *Creation of decision trees*: Various decision trees are created using the bagging technique, which means bootstrap aggregated, bootstrap aggregation. With bagging, the algorithm generates multiple subsets of the training data, using different combinations of the available features or variables. The training process is repeated for each data set and an individual decision tree is obtained.\n",
    "3. *Prediction*: Once all the decision trees have been created, the predictions of each one are added to make a final prediction. In the prediction stage, a new observation, a set of features, is passed through each of the decision trees, which make their own individual prediction. The result of each of these predictions is averaged to produce a more accurate prediction.\n",
    "4. *Correlation reduction*: The Random Forest algorithm reduces the correlation between decision trees, one of its greatest benefits, by randomly selecting features and training records.\n",
    "\n",
    "In conclusion, the Random Forest algorithm takes advantage of the bootstrap technique to create a multitude of training data subsets and build individual decision trees in each of them. The trees are combined last to provide a more accurate and stable prediction. The technique performs very well on large data sets with many variables, being widely used in classification and regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "With our data set, the first thing we need to do is identify who is the set $X$ *(properties)* and who is the set $y$ *(categories)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class column is excluded, since it is more of a category\n",
    "X = df_iris.drop(columns=['class'])\n",
    "\n",
    "# Take only the possible categories of iris\n",
    "y = df_iris[\"class\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test data\n",
    "\n",
    "One of the fundamental steps when creating a machine learning model is to split the data set into two sets: one for training and one for testing. It is essential that these two sets are balanced, which means that the model must correctly identify the variables of interest without incurring in overfitting or underfitting problems.\n",
    "\n",
    "To split the dataset into the two sets, the `train_test_split` method of the `sklearn` library is used. This method allows you to separate the data into four different sets: training and testing sets for the independent variables `X` and training and testing sets for the dependent variable `y`. This step is crucial to validate the model's ability to adapt to new data and ensure its effectiveness in decision making.\n",
    "\n",
    "Notice that we start with two sets ($X$ and $y$) and come out with four ($X_{train}, X_{test}, y_{train}, y_{test}$). In this case, the training set corresponds to 75\\% of the total data and the rest is the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=50, shuffle=True)\n",
    "print(f'Total data training: {len(X_train)}\\n'\n",
    "      f'Full test data: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some characteristics can be changed in the division of the data, for example, what amount we want for the training set and for the test set, in the case above, we chose 25% of the data for the test set `test_size=0.25 `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model\n",
    "\n",
    "The next step is to create the model, which is called `RandomForestClassifier`, it comes with some default parameters (number of trees, depth, among others, which can be changed when it is being created. For more information on the parameters see recommended to see the description of the library [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the RandomForestClassifier libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# The Random Forest classification model is instantiated, the Random Forest algorithm uses\n",
    "# decision trees, in this case we are going to use 100 of these and only three levels deep\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Now, the next step is to fit the model and generate predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the model\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that we train our `forest` classification algorithm, which can be used to predict the class of iris by knowing its `sepal_length`, `sepal_width` and `petal_width` features (remember we removed one of the features for having a high correlation with other)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the model\n",
    "\n",
    "The interesting thing about decision trees is that we can visually represent how the training decisions were made. In the following code we will see what the first decision tree did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a tree of the random forest model\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(forest.estimators_[0], filled=True, feature_names=list(X.columns))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree algorithm involves a series of questions or nodes that help classify the data. The first node asks whether the `sepal_width` characteristic is less than or equal to 3.15. This question divides the data set of 76 samples into two sets, one consisting of 47 samples and one consisting of 29 samples. Each new node then asks another question to continue the division until each of the 76 samples falls into one of the three categories.\n",
    "\n",
    "For example, starting from the first node (`sepal_width` $<=3.15$), we continue with those that meet this condition, reaching the node on the left (`sepal_length` $<=6.15$). Then, we continue again with those that meet this condition and arrive at the node on the left (`sepal_width` $<=2.95$). Finally, we continue with those that did not meet that condition and we see that 10 of the 76 initial data are classified with the first label, which in this case is `iris setosa`.\n",
    "\n",
    "Let us remember that this is done with 100 decision trees, each one with different samples, which means that the questions of the nodes may also be different or at least not in the same order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prediction\n",
    "\n",
    "In this section we focus on evaluating the performance of the model in terms of its ability to make accurate predictions on elements outside its training set. To achieve this, we use the test set (`X_test`, `y_test`), which contains data that the model has not previously seen.\n",
    "\n",
    "Before evaluating the performance of the model, it is important to choose an appropriate metric. In this case, when working with a classification model, a commonly used metric is **accuracy**, which measures the proportion of correct predictions made by the model.\n",
    "\n",
    "To evaluate the performance of the model on the test set, we first make use of the algorithm to make predictions on the data in `X_test`. Then, we use the true category (`y_test`) to compare it with the predicted category and calculate the accuracy.\n",
    "\n",
    "It should be noted that the accuracy calculates the prediction as follows: \"the set of labels predicted for a sample must exactly match the corresponding set of labels in `y_true`\". In other words, the model is evaluated on its ability to accurately and precisely predict the correct class of each item in the test set.\n",
    "\n",
    "It is important to mention that the accuracy is only one of the different metrics that can be used to evaluate the performance of the model, and that it is advisable to explore other metrics depending on the type of problem and the specific objectives of the investigation. In addition, it is important to use various validation techniques to ensure the reliability and validity of the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# The predictions are generated with the test set\n",
    "predictions = forest.predict(X_test)\n",
    "\n",
    "# Check the accuracy of the generated model\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our generated model has an accuracy of **94.7\\%**, which means that the prediction was correct 94.7\\% of the time, regardless of the category.\n",
    "One way to understand the metric is that: out of 100 Iris flower data where all three characteristics are known, approximately 95 will be correctly classified.\n",
    "\n",
    "And that's basically it. Step by step what we did was: \n",
    "\n",
    "a) train/fit a model with the training set, \n",
    "\n",
    "b) predict the classes of the test set data, \n",
    "\n",
    "c) compare the predictions with reality and measure it with the metric called `accuracy `."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other evaluation metrics\n",
    "We can evaluate the previously generated model, with the review of other indicators such as **precision**, **recall** and **F1 score**, generated by means of the `classification_report()` method. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final considerations\n",
    "In this content, we address the first introductory step for training classification models in the field of artificial intelligence and machine learning. The main objective is to develop effective prediction algorithms that allow classifying new unknown data based on a previously labeled data set. It is important to bear in mind that there are still many aspects to be addressed and others that will not be covered in depth.\n",
    "\n",
    "It is worth noting the uniqueness of each model and the need to apply different training techniques as appropriate to obtain optimal results in each case. In the field of evaluation of classification models, there are numerous metrics that must be considered depending on the particular characteristics of the data set and the type of model used. Therefore, it is essential to have a solid understanding of the different evaluation options in order to select the most appropriate one in each situation.\n",
    "\n",
    "In the hands-on section, we'll take a closer look at a binary classification model and use various specialized evaluation tools and metrics to improve the effectiveness of this specific type of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Burkov, A. (2019). The Hundred-Page Machine Learning Book. Andriy Burkov\n",
    "2. Theobald, O. (2019). Machine Learning for Absolute Beginners: A Plain English Introduction (3rd ed.). Scatterplot Press"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
