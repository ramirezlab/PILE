{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al Aprendizaje Automático\n",
    "\n",
    "> **Nota:** Este libro esta disponible de dos maneras: \n",
    "> 1. Descargando el repositorio y siguiendo las instrucciones que estan en el archivo [README.md](https://github.com/ramirezlab/CHEMO/blob/main/README.md)\n",
    "> 2. Haciendo clic aquí en [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ramirezlab/CHEMO/blob/main/3_PART_THREE/3.1_Introduccion_ML.es.ipynb?hl=es)\n",
    "\n",
    "El Aprendizaje Automático, conocido como Machine Learning en inglés, es un campo de la computación que utiliza algoritmos para permitir a las máquinas aprender a partir de los datos, sin ser programadas de manera explícita para ello <sup> **1** </sup>. Es una de las bases de las tecnologías modernas, impulsando desde los motores de búsqueda hasta sistemas de reconocimiento de voz y objetos.\n",
    "\n",
    "En este notebook, se emplearán algoritmos de aprendizaje para reconocer patrones de un determinado conjunto de datos. Luego de ser entrenados, estos algoritmos podrán realizar predicciones precisas para nuevos datos, gracias al ajuste de sus parámetros durante el proceso de entrenamiento.\n",
    "\n",
    "Existe una amplia variedad de algoritmos de ML, cada uno adecuado para distintos tipos de problemas. En general, estos algoritmos se agrupan en tres categorías principales: Aprendizaje Supervisado, Aprendizaje No Supervisado y Aprendizaje por Refuerzo. Cada uno de ellos posee sus propias características y aplicaciones particulares.\n",
    "\n",
    "\n",
    "## Aprendizaje Supervisado\n",
    "\n",
    "El Aprendizaje Supervisado es el proceso mediante el cual se utilizan datos etiquetados para entrenar un modelo de ML. Estos datos etiquetados consisten en un conjunto de observaciones y sus correspondientes resultados, y son utilizados para enseñar al modelo a predecir futuros resultados precisos <sup> **1, 2** </sup>. Se trata de un proceso altamente eficaz, y se utiliza comúnmente en tareas de clasificación y regresión.\n",
    "\n",
    "\n",
    "## Aprendizaje No Supervisado\n",
    "\n",
    "El Aprendizaje No Supervisado es una técnica de ML utilizada cuando no se dispone de datos etiquetados. En lugar de utilizar observaciones etiquetadas, el modelo de ML explora los patrones subyacentes en los datos y los utiliza para hacer predicciones <sup> **1, 2** </sup>. Esta técnica se utiliza comúnmente en la segmentación de clientes y la detección de anomalías.\n",
    "\n",
    "## Aprendizaje por Refuerzo\n",
    "\n",
    "El Aprendizaje por Refuerzo implica la enseñanza del modelo de ML cómo tomar decisiones en un ambiente dinámico, a través del ensayo y error. El modelo de ML utiliza la retroalimentación concedida por su ambiente para ajustar su comportamiento y mejorar sus decisiones futuras <sup> **1, 2** </sup>. Esta técnica se utiliza comúnmente en la robótica y los juegos de computadora.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Clasificación y Regresión\n",
    "\n",
    "Los modelos de clasificación y regresión son dos técnicas de Aprendizaje Supervisado muy utilizadas en el campo del Aprendizaje Automático. Estos modelos permiten predecir un resultado a partir de un conjunto de variables de entrada, utilizando datos de entrenamiento previamente proporcionados.\n",
    "\n",
    "## Modelos de Regresión\n",
    "La Regresión es un tipo de Aprendizaje Supervisado que se utiliza para predecir valores continuos <sup> **1** </sup>. En otras palabras, se utiliza para predecir un valor en función de una o más variables de entrada.\n",
    "\n",
    "Por ejemplo, considere el conjunto de datos de precios de casas, donde el objetivo es predecir el precio de una casa en función de características como el área, el número de habitaciones, etc. En este caso, la Regresión se utiliza para predecir un valor continuo (el precio de la casa) en función de las variables de entrada.\n",
    "\n",
    "Existen muchos tipos diferentes de Regresión, incluyendo la Regresión Lineal, la Regresión Polinómica y la Regresión Logística. Cada tipo de Regresión tiene sus propias suposiciones y limitaciones, y la elección de una técnica de Regresión depende del problema específico que se esté resolviendo.\n",
    "\n",
    "En el campo de la bioinformática, estos modelos pueden utilizarse para predecir la relación existente entre la cantidad de ciertas sustancias químicas en una célula y su respuesta a diferentes situaciones. Además, estos modelos pueden ser empleados en la física y la química para predecir cómo una reacción química puede cambiar a medida que cambian ciertos factores externos. Algunos algoritmos comunes de regresión incluyen Regresión Lineal, Regresión Polinómica y Regresión Robusta.\n",
    "\n",
    "## Modelos de Clasificación\n",
    "La Clasificación es otro tipo de Aprendizaje Supervisado que se utiliza para predecir valores discretos (etiquetas) <sup> **1** </sup>. En otras palabras, se utiliza para predecir una etiqueta en función de una o más variables de entrada.\n",
    "\n",
    "Por ejemplo, considere el conjunto de datos que contiene información sobre automóviles, donde el objetivo es predecir si un automóvil es “seguro” o “no seguro” en función de características como el tamaño, la potencia, etc. En este caso, la Clasificación se utiliza para predecir una etiqueta discreta (seguro o no seguro) en función de las variables de entrada.\n",
    "\n",
    "Existen muchos tipos diferentes de Clasificación, incluyendo Árboles de Decisión, Máquinas de Vectores de Soporte (SVMs) y Redes Neuronales. Cada tipo de Clasificación tiene sus propias suposiciones y limitaciones, y también la elección de una técnica de Clasificación depende del problema específico que se esté resolviendo.\n",
    "\n",
    "Los modelos de clasificación son utilizados comúnmente en bioinformática para clasificar distintos tipos de secuencias de ADN o proteínas. Además, son herramientas vitales en el campo de la biología celular para ayudar a identificar distintos tipos de células en un tejido. En las ciencias ambientales, los modelos de clasificación se utilizan para clasificar distintos tipos de animales o plantas. Algunos algoritmos comunes de clasificación incluyen Árboles de Decisión, Regresión Logística y Máquinas de Soporte Vectorial.\n",
    "\n",
    "## Modelos de Clasificación vs Modelos de Regresión\n",
    "Mientras que los modelos de clasificación se enfocan en predecir valores categóricos, los modelos de regresión se enfocan en predecir valores numéricos. Esto significa que los modelos de clasificación son utilizados para resolver problemas de clasificación, mientras que los modelos de regresión son empleados para problemas de predicción de valores numéricos.\n",
    "\n",
    "En conclusión, los modelos de clasificación y regresión son técnicas fundamentales en el campo de la bioinformática y las ciencias en general. Estos modelos permiten predecir resultados precisos a partir de conjuntos de datos de entrada previamente proporcionados. Para los estudiantes de estas disciplinas, es importante tener un buen conocimiento de estas técnicas y de cómo aplicarlas en diferentes problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuestro primer modelo de aprendizaje automático\n",
    "\n",
    "La idea ahora es ir paso a paso desarrollando un modelo muy simple de *clasificación*. Durante este proceso haremos uso de las librerías de `Pandas` y `Numpy` e introduciremos una nueva que se llama `scikit-learn`.\n",
    "\n",
    "Empecemos descargando los datos con los que vamos a trabajar. Para el desarrollo de nuestro modelo vamos a utilizar el clásico dataset de [iris](https://archive-beta.ics.uci.edu/dataset/53/iris):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importa Pandas y se le asigna el alias pd\n",
    "import pandas as pd\n",
    "\n",
    "# Se importa los datos del dataset y se convierten en un DataFrame.\n",
    "df_iris = pd.read_csv(\"https://raw.githubusercontent.com/ramirezlab/CHEMO/main/3_PART_THREE/data/iris.csv\", names=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"])\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora que tenemos en el conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos 150 observaciones en total (`count`), el conjunto tiene cuatro características (columnas: `sepal_lenght`, `sepal_width`, `petal_lenght` y `petal width`) y una clasificación de clase de iris(columna `class`), además, como las características del cunjunto son numéricas, podemos ver algunas estadísticas relevantes como la media, desviación estándar y algunos datos por percentiles. Más allá de eso vemos que nuestro conjunto muy bien organizado y a primera vista no parece tener ningún valor que vaya a ser un problema. ¿Podemos saber cuántas clases de iris fueron observadas? ¡Claro que sí!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Como podemos ver, el método `value_counts()` nos dice que son 3 clases diferentes de iris, cada una con respectivamente 50 observaciones (esto se conoce como un conjunto de datos *balanceado*). Veámoslo gráficamente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importa la libreria Seaborn y se le asigna el alias sns\n",
    "import seaborn as sns\n",
    "\n",
    "# Se visualiza la columna class del DataFrame\n",
    "sns.countplot(x=df_iris['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos de suerte, nuestros datos ya se encuentran balanceados, y en buena medida esto es importante en un problema de *clasificación*, pero en la vida real es más realista lidiar con conjuntos de datos que no se encuentran balanceados.\n",
    "\n",
    "> Si los datos no están balanceados, nos llevará a modelos no balanceados que tendrían muy bajo funcionamiento o poder de predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlaciones y algo más\n",
    "\n",
    "A continuación, usaremos el poder de la visualización para descubrir si tenemos algún patrón o correlación, en algunas de las características del iris sobre el conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importa Seaborn, y se visualizan los datos en un pairplot.\n",
    "import seaborn as sns \n",
    "sns.pairplot(df_iris, hue=\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizando un poco las gráficas, podemos ver que la mayoría de las características *(Ancho y largo de pétalo, largo y ancho de sépalo) * siguen una **distribución normal**, lo que significa que la mayoría de las observaciones se localizan al centro de los datos, por medio de esta lectura podríamos pensar cuál sería el mejor algoritmo para este problema.\n",
    "\n",
    "También podemos ver que algunas clases de Iris, y en ciertas características los datos no se sobreponen, lo que nos dice que está bien que usemos la mayoría de las columnas posibles para tratar de evitar algún dato mentiroso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora el siguiente paso es sobre las correlaciones que pueden existir entre las características y para ello haremos uso del **mapa de calor**, *por su nombre en inglés Heat Map*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figure, axis = plt.subplots()\n",
    "\n",
    "sns.heatmap(df_iris.corr(numeric_only=True), annot=True, fmt=\".2f\", ax=axis)\n",
    "\n",
    "plt.title('Iris Correlation Map')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al analizar el conjunto de datos, se ha identificado que las características independientes, como el largo y ancho del pétalo o el largo y ancho del sépalo, presentan un alto grado de correlación. Este fenómeno se conoce como correlación múltiple y puede llevar a resultados poco confiables al realizar análisis estadísticos inferenciales.\n",
    "\n",
    "En este caso, en particular, se ha identificado que la característica `petal_length` tiene una correlación bastante elevada con otras características. Por esta razón, para evitar posibles problemas debidos a la correlación múltiple, se recomienda eliminar `petal_length` del conjunto de datos.\n",
    "\n",
    "Es importante realizar un análisis detallado de las características independientes para identificar cualquier posible correlación y determinar las soluciones adecuadas para reducir la complejidad del conjunto de datos. En algunos casos, la eliminación de características altamente correlacionadas puede resultar en una disminución significativa en la complejidad, lo que puede mejorar la calidad de los resultados al realizar el análisis de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.drop('petal_length', axis=1, inplace=True)\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de Clasificación: Random Forest Classifier\n",
    "\n",
    "## Introducción\n",
    "\n",
    "El algoritmo Random Forest, también conocido como “bosque aleatorio”, se emplea en el aprendizaje supervisado para la clasificación, regresión y otras tareas de minería de datos. Esta técnica de ensamblaje combina varios árboles de decisión independientes para crear un modelo singular mejorado.\n",
    "\n",
    "\n",
    "## ¿Cómo funciona el algoritmo de Random Forest?\n",
    "Para entender la forma en que funciona el algoritmo de Random Forest, primero hay que considerar un modelo más simple, el árbol de decisión. Este último utiliza lógica del tipo “si/entonces” para tomar decisiones, dividiendo el conjunto de datos en subconjuntos más pequeños utilizando una serie de preguntas simples, conocidas como “nodos”, para llegar a una predicción final. Así, a partir de un conjunto de datos de entrenamiento se utilizan reglas lógicas para predecir el valor de destino de nuevos conjuntos de características. El algoritmo de Random Forest implementa este modelo, pero con múltiples árboles de decisión que toman aleatoriamente una parte del conjunto de datos de entrenamiento para luego votar en la clasificación <sup> **1** </sup>. Para ilustrar los pasos del algoritmo, se describen puntualmente en el siguiente apartado.\n",
    "\n",
    "1. *Selección de los datos de entrenamiento*: Se toma una muestra aleatoria de tamaño N de los datos de entrenamiento originales, lo que se denomina muestreo aleatorio de arranque o Bootstrap. A partir de esa muestra se crea un árbol de decisión.\n",
    "2. *Creación de árboles de decisión*: Se crean diversos árboles de decisión mediante la técnica bagging, que significa bootstrap aggregated, agregación bootstrap. Con bagging, el algoritmo genera múltiples subconjuntos de datos de entrenamiento, utilizando diferentes combinaciones de las características o variables disponibles. El proceso de entrenamiento se repite para cada conjunto de datos y se obtiene un árbol de decisión individual.\n",
    "3. *Predicción*: Una vez creados todos los árboles de decisión, se suman las predicciones de cada uno para hacer una predicción final. En la etapa de predicción se pasa una nueva observación, un conjunto de características, por cada uno de los árboles de decisión, los cuales elaboran su propia predicción individual. El resultado de cada una de estas predicciones se promedia para generar una predicción más precisa.\n",
    "4. *Reducción de la correlación*: El algoritmo Random Forest reduce la correlación entre los árboles de decisión, uno de sus mayores beneficios, mediante la selección aleatoria de características y registros de entrenamiento.\n",
    "\n",
    "En conclusión, el algoritmo de Random Forest aprovecha la técnica de bootstrap para crear multitud de subconjuntos de datos de entrenamiento y construir árboles de decisión individuales en cada uno de ellos. Los árboles se combinan al final para proporcionar una predicción más precisa y estable. La técnica se desempeña muy bien en grandes conjuntos de datos con muchas variables, siendo utilizado ampliamente en problemas de clasificación y regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Entrenamiento del modelo\n",
    "\n",
    "Con nuestro conjunto de datos, lo primero que debemos hacer es identificar quien es el conjunto $X$ *(propiedades)* y quien es el conjunto $y$ *(categorías)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se excluye la columna clase, dado que es más una categoría\n",
    "X = df_iris.drop(columns=['class'])\n",
    "\n",
    "# Se toman solo las posibles categorías de iris\n",
    "y = df_iris[\"class\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos de entrenamiento y de prueba\n",
    "\n",
    "Uno de los pasos fundamentales al crear un modelo de aprendizaje automático es dividir el conjunto de datos en dos conjuntos: uno de entrenamiento y otro de prueba. Es esencial que estos dos conjuntos estén equilibrados, lo que significa que el modelo debe identificar correctamente las variables de interés sin incurrir en problemas de sobreenajuste o subajuste.\n",
    "\n",
    "Para dividir el conjunto de datos en los dos conjuntos, se utiliza el método `train_test_split` de la librería `sklearn`. Este método permite separar los datos en cuatro conjuntos diferentes: conjuntos de entrenamiento y prueba para las variables independientes `X` y conjuntos de entrenamiento y prueba para la variable dependiente `y`. Este paso es crucial para validar la capacidad del modelo de adaptarse a nuevos datos y garantizar su efectividad en la toma de decisiones.\n",
    "\n",
    "Notemos que comenzamos con dos conjuntos ($X$ y $y$) y salimos con cuatro ($X_{train}, X_{test}, y_{train}, y_{test}$). En esta caso, el conjunto de entrenamiento corresponde al 75\\% del total de datos y el restante es el conjunto de prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=50, shuffle=True)\n",
    "print(f'Total datos entrenamiento: {len(X_train)}\\n'\n",
    "      f'Total datos de prueba: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pueden cambiar algunas características en la división de los datos, por ejemplo, qué cantidad queremos para el conjunto de entrenamiento y para el de prueba, en el caso de arriba, escogimos 25% de los datos para el conjunto de prueba `test_size=0.25`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciar el modelo\n",
    "\n",
    "El siguiente paso es crear el modelo, el cual se llama `RandomForestClassifier`, este viene con unos parámetros por defecto (número de árboles, profundidad, entre otros, los cuales se pueden cambiar cuando se está creando. Para mayor información de los parámetros se recomienda ver la descripción de la librería [aquí](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importan las librerias de RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Se crea la instancia del modelo de clasificación Random Forest, el algoritmo de Random Forest usa\n",
    "# árboles de decisión, en este caso vamos a utilizar un 100 de estos y solamente tres niveles de profundidad\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar el modelo\n",
    "\n",
    "Ahora, el siguiente paso es ajustar el modelo y generar predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se ajusta (fit) el modelo\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y así de fácil entrenamos nuestro algoritmo de clasificación `forest`, el cual puede usarse para predecir la clase de iris conociendo sus características `sepal_length`, `sepal_width` y `petal_width` (recordemos que eliminamos una de las características por tener una alta correlación con otra)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizar el modelo\n",
    "\n",
    "Lo interesante de los árboles de decisión es que podemos representar visualmente cómo se tomaron las decisiones del entrenamiento. En el siguiente código vamos a ver qué fue lo que hizo el primer árbol de decisión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar un árbol del modelo de random forest\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(forest.estimators_[0], filled=True, feature_names=list(X.columns))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de árbol de decisión implica una serie de preguntas o nodos que ayudan a clasificar los datos. El primer nodo plantea la pregunta de si la característica `sepal_width` es menor o igual a 3.15. Esta pregunta divide el conjunto de datos de 76 muestras en dos conjuntos, uno que consta de 47 muestras y otro que consta de 29 muestras. Cada nuevo nodo luego hace otra pregunta para continuar la división hasta que cada una de las 76 muestras se clasifica en una de las tres categorías.\n",
    "\n",
    "Por ejemplo, partiendo del primer nodo (`sepal_width` $<=3.15$), continuamos con aquellos que cumplen esta condición, llegando al nodo de la izquierda (`sepal_length` $<=6.15$). Luego, continuamos nuevamente con aquellos que cumplen esta condición y llegamos al nodo de la izquierda (`sepal_width` $<=2.95$). Finalmente, continuamos con aquellos que no cumplieron esa condición y vemos que 10 de los 76 datos iniciales se clasifican con la primera etiqueta, que en este caso es `iris setosa`.\n",
    "\n",
    "Recordemos que esto se hace con 100 árboles de decisión, cada uno con muestras diferentes, lo que hace que posiblemente las preguntas de los nodos también sean diferentes o al menos no estén en el mismo orden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción del modelo\n",
    "\n",
    "En esta sección nos enfocamos en evaluar el rendimiento del modelo en términos de su capacidad para realizar predicciones precisas sobre elementos externos a su conjunto de entrenamiento. Para lograr esto, utilizamos el conjunto de prueba (`X_test`, `y_test`), el cual contiene datos que el modelo no ha visto previamente.\n",
    "\n",
    "Antes de evaluar el rendimiento del modelo, es importante escoger una métrica adecuada. En este caso, al estar trabajando con un modelo de clasificación, una métrica comúnmente utilizada es el **accuracy**, que mide la proporción de predicciones correctas realizadas por el modelo.\n",
    "\n",
    "Para evaluar el desempeño del modelo en el conjunto de prueba, primero hacemos uso del algoritmo para realizar predicciones sobre los datos en `X_test`. Luego, utilizamos la categoría verdadera (`y_test`) para compararla con la categoría predicha y calcular el accuracy.\n",
    "\n",
    "Cabe resaltar que el accuracy calcula la predicción de la siguiente manera: \"el conjunto de etiquetas predichas para una muestra debe coincidir exactamente con el conjunto correspondiente de etiquetas en `y_true`\". En otras palabras, el modelo es evaluado en su capacidad para predecir de manera precisa y exacta la clase correcta de cada elemento en el conjunto de prueba.\n",
    "\n",
    "Es importante mencionar que el accuracy es solamente una de las distintas métricas que pueden ser utilizadas para evaluar el rendimiento del modelo, y que es recomendable explorar otras métricas dependiendo del tipo de problema y los objetivos específicos de la investigación. Además, es importante utilizar diversas técnicas de validación para asegurar la fiabilidad y validez de los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Se generan las predicciones con el conjunto de pruebas\n",
    "predictions = forest.predict(X_test)\n",
    "\n",
    "# Se verifica la precision del modelo generado\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro modelo generado tiene una exactitud del **94.7\\%**, lo que significa que la predicción fue correcta el 94.7\\% de las veces, sin importar la categoría.\n",
    "Una manera de entender la métrica es que: de cada 100 datos de flores Iris donde se conocen las tres características, aproximadamente 95 estarán correctamente clasificados.\n",
    "\n",
    "Y eso es básicamente todo. Paso a paso lo que hicimos fue: \n",
    "\n",
    "a) entrenar/ajustar un modelo con el conjunto de entrenamiento, \n",
    "\n",
    "b) predecir las clases de los datos del conjunto de prueba, \n",
    "\n",
    "c) comparar las predicciones con la realidad y medirla con la métrica llamada `accuracy`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otras métricas de evaluación \n",
    "Podemos evaluar el modelo previamente generado, con la revisión de otros indicadores como lo son la **precisión**, **el recuerdo** y la **puntuación F1**, generados por medio del método `classification_report()` para ello:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideraciones finales\n",
    "En este contenido, abordamos el primer paso introductorio para el entrenamiento de modelos de clasificación en el campo de la inteligencia artificial y el aprendizaje automático. El objetivo principal es desarrollar algoritmos de predicción efectivos que permitan clasificar nuevos datos desconocidos con base en un conjunto de datos previamente etiquetados. Es importante tener en cuenta que todavía hay muchos aspectos por abordar y otros que no serán tratados en profundidad.\n",
    "\n",
    "Cabe destacar la singularidad de cada modelo y la necesidad de aplicar diferentes técnicas de entrenamiento según corresponda para obtener resultados óptimos en cada caso. En el ámbito de la evaluación de modelos de clasificación, existen numerosas métricas que deben ser consideradas en función de las características particulares del conjunto de datos y del tipo de modelo utilizado. Por lo tanto, es fundamental tener un conocimiento sólido sobre las diferentes opciones de evaluación para seleccionar la más adecuada en cada situación.\n",
    "\n",
    "En la sección práctica, abordaremos en detalle un modelo de clasificación binaria y utilizaremos diversas herramientas y métricas de evaluación especializadas para mejorar la efectividad de este tipo específico de modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "1. Burkov, A. (2019). The Hundred-Page Machine Learning Book. Andriy Burkov\n",
    "2. Theobald, O. (2019). Machine Learning for Absolute Beginners: A Plain English Introduction (3rd ed.). Scatterplot Press"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "fee31c63b4acacf7632ca8e3d61ceb5fcbb4c0c0b5cdfa424159c0350f34c501"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
